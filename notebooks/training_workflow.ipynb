{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **`word2vec` Training Workflow**\n",
    "In this workflow, we first train `word2vec` models across years using a range of hyperparameters (e.g., vector dimensions and training epochs). The purpose is twofold: (1) to determine whether models from earlier years are reasonably stable, and (2) choose a set of hyperparameters that yield good results across all years. Models are evaluated using \"intrinsic\" tests of similarity and analogy performance, which we visualize using plots and analyze using linear regression.\n",
    "\n",
    "Once we've chosen our hyperparameters, we use them to train models for every year from 1900 through 2019."
   ],
   "id": "cc76a8af4b09812b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "67f4fbe53cf4dc6f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T02:15:31.929922Z",
     "start_time": "2025-11-17T02:15:29.649484Z"
    }
   },
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2\n\nfrom ngramkit.ngram_train.word2vec import build_word2vec_models, evaluate_word2vec_models, plot_evaluation_results\nfrom ngramkit.ngram_train.word2vec import run_regression_analysis, plot_regression_results",
   "id": "b68550efb08f6107",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Configure",
   "id": "10e5752e64b0b3cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T02:15:32.071323Z",
     "start_time": "2025-11-17T02:15:31.935421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "release = '20200217'\n",
    "language = 'eng-fiction'\n",
    "size = 5"
   ],
   "id": "1bcde7a8fb7855d1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Test Model Hyperparameters**\n",
    "### Train Models\n",
    "Here we test models from 1900 tp 2015 in 5-year increments, cycling through a range of reasonable hyperparameters. In this workflow, we constrain our grid search as follows:\n",
    "1. We stick to the Skip-Gram (`skip-gram`) approach. Skip-gram is known to be more efficient than Continuous Bag of Words (`CBOW`) for Google n-gram data.\n",
    "2. We test vector dimensions (`vector_size`) from 100 to 300. Our vocabulary is probably too small to support the extraction of more than 300 meaningful features.\n",
    "3. We test training epochs (`epochs`) from 5 to 30. More than 30 epochs risks overfitting.\n",
    "4. We set the minimum word count (`min_count`) to 1, meaning that no words will be excluded from training. Our whitelist ensures that all vocabulary words appear frequently in every corpus from 1900 to 2015.\n",
    "5. Weighting (`weight_by`) is set to none. `word2vec` already downweights extremely frequent words.\n",
    "6. We set a context window (`window`) of 4. This width extracts as much context as possible from 5-grams."
   ],
   "id": "aeb725b17d9628ba"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-17T02:15:35.945574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "build_word2vec_models(\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    dir_suffix='test',\n",
    "    years=(1900, 2015),\n",
    "    year_step=5,\n",
    "    weight_by=('none',),\n",
    "    vector_size=(100, 200, 300),\n",
    "    window=(4,),\n",
    "    min_count=(1,),\n",
    "    approach=('skip-gram',),\n",
    "    epochs=(5, 10, 15, 20, 25, 30),\n",
    "    max_parallel_models=25,\n",
    "    workers_per_model=2,\n",
    "    mode=\"resume\",\n",
    "    unk_mode=\"strip\",\n",
    "    use_corpus_file=True,\n",
    "    cache_corpus=True\n",
    ")"
   ],
   "id": "cae4c8204dfaefe0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning for existing models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning existing models: 100%|██████████| 29/29 [00:00<00:00, 43.21 files/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Valid models found:    29\n",
      "  Invalid/partial:       0\n",
      "\n",
      "WORD2VEC MODEL TRAINING\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-16 21:15:36\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Database:             ...NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db\n",
      "Model directory:      ...edk202/NLP_models/Google_Books/20200217/eng-fiction/5gram_files/models_test\n",
      "Log directory:        ...NLP_models/Google_Books/20200217/eng-fiction/5gram_files/logs_test/training\n",
      "Parallel models:      25\n",
      "\n",
      "Training Parameters\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Years:                1900–2015 (step=5, 24 years)\n",
      "Weighting:            ('none',)\n",
      "Vector size:          (100, 200, 300)\n",
      "Context window:       (4,)\n",
      "Minimum word count:   (1,)\n",
      "Approach:             ('skip-gram',)\n",
      "Training epochs:      (5, 10, 15, 20, 25, 30)\n",
      "\n",
      "Data Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "UNK mode:             strip\n",
      "Corpus mode:          Shared corpus file (one per year/weight)\n",
      "Workers per model:    2\n",
      "\n",
      "Execution\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Total models in grid: 432\n",
      "Existing valid:       29\n",
      "Models to train:      403\n",
      "Parameter combos:     18\n",
      "Years:                24\n",
      "\n",
      "Creating corpus files in parallel...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Created corpus file for year 1945, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1945_wbnone_es6m8c9i.txt\n",
      "  Created corpus file for year 1940, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1940_wbnone_c_6xu_1a.txt\n",
      "  Created corpus file for year 1935, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1935_wbnone_s93eev6x.txt\n",
      "  Created corpus file for year 1955, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1955_wbnone_fmcs9xuk.txt\n",
      "  Created corpus file for year 1915, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1915_wbnone_6p58gn_w.txt\n",
      "  Created corpus file for year 1965, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1965_wbnone_yv_2g254.txt\n",
      "  Created corpus file for year 1975, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1975_wbnone_kd8s5naj.txt\n",
      "  Created corpus file for year 1930, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1930_wbnone_u5bv6pq0.txt\n",
      "  Created corpus file for year 1960, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1960_wbnone_ipxcih7k.txt\n",
      "  Created corpus file for year 1970, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1970_wbnone_pe4oy6_2.txt\n",
      "  Created corpus file for year 1925, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1925_wbnone_wmjl1jjn.txt\n",
      "  Created corpus file for year 1980, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1980_wbnone_e3wwokmo.txt\n",
      "  Created corpus file for year 1920, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1920_wbnone_7kyyaac6.txt\n",
      "  Created corpus file for year 1950, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1950_wbnone_rwb7v9g2.txt\n",
      "  Created corpus file for year 1905, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1905_wbnone_dx9plhdq.txt\n",
      "  Created corpus file for year 1985, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1985_wbnone_f_m1vwei.txt\n",
      "  Created corpus file for year 1910, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1910_wbnone_ethehn_i.txt\n",
      "  Created corpus file for year 1990, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1990_wbnone_iquoeep4.txt\n",
      "  Created corpus file for year 1900, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1900_wbnone_qmc2kv5l.txt\n",
      "  Created corpus file for year 1995, weight_by=none: /state/partition1/job-457422/w2v_corpus_y1995_wbnone_huu2moq_.txt\n",
      "  Created corpus file for year 2000, weight_by=none: /state/partition1/job-457422/w2v_corpus_y2000_wbnone_eafiao5k.txt\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Evaluate Models\n",
    "\n",
    "Here we evaluate the models we've trained using two \"intrinsic\" tests: (1) a _similarity test_ assessing how well each model predicts human-rated synonymy judgments, and (2) an _analogy test_ assessing how well each model can answer SAT-style analogy questions. Test results are saved to a CSV file.\n",
    "\n",
    "Similarity performance is the metric of choice for models intended to track semantic relatedness over time. However, we run both tests here to demonstrate the evaluation code and show that different hyperparameters lend themselves to different performance metrics."
   ],
   "id": "9c09a36e696c9ccf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T00:52:25.607679Z",
     "start_time": "2025-11-11T00:19:38.877884Z"
    }
   },
   "cell_type": "code",
   "source": "evaluate_word2vec_models(\n    ngram_size=size,\n    repo_release_id=release,\n    repo_corpus_id=language,\n    db_path_stub=db_path_stub,\n    dir_suffix='test',\n    save_mode='overwrite',\n    run_similarity=True,\n    run_analogy=True,\n    workers=100\n)",
   "id": "c82c892c2bc6a2fc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Visualize Model Performance\n",
    "The code below plots the results of the similarity and analogy tests for easy inspection.\n",
    "#### Similarity Results\n",
    "Visual inspection of the similarity plots reveals that all models display decent performance, with the earliest models scoring in the .50–.55 range and the most recent models scoring in the .58–.62 range over our chose timespan. Overall, then, the models are performing well for ngram-based semantic relatedness. Vector dimensions and training epochs don't seem to matter much overall."
   ],
   "id": "85be73929b1d7ea"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-10T19:09:33.828634Z",
     "start_time": "2025-11-10T19:09:31.784046Z"
    }
   },
   "cell_type": "code",
   "source": "from pathlib import Path\nfrom ngramkit.ngram_acquire.db.build_path import build_db_path\nfrom ngramkit.ngram_train.word2vec.config import construct_model_path\n\n# Construct path to evaluation results\nbase_path = Path(build_db_path(db_path_stub, size, release, language)).parent\nmodel_base = construct_model_path(str(base_path))\neval_file = Path(model_base) / \"evaluation_results_test.csv\"\n\nplot_evaluation_results(\n    csv_file=str(eval_file),\n    verbose=False,\n    metric='similarity_score',\n    x_vars=['epochs', 'vector_size'],\n    panel_by='year',\n    plot_type='line',\n    plot_title='Similarity Score by Training Epochs and Vector Size'\n)",
   "id": "dc4852052e54eb41",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Analogy Results\n",
    "Like the similarity results, model performance improves with corpus recency. Unlike the similarity results, analogy performance clearly improves with the number training epochs. The number of vector dimensions doesn't seem to matter much."
   ],
   "id": "a4e462230bc10d43"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:25:29.286064Z",
     "start_time": "2025-11-11T02:25:27.310983Z"
    }
   },
   "cell_type": "code",
   "source": "# eval_file already constructed above\nplot_evaluation_results(\n    csv_file=str(eval_file),\n    verbose=False,\n    metric='analogy_score',\n    x_vars=['epochs', 'vector_size'],\n    panel_by='year',\n    plot_type='line',\n    plot_title='Analogy Score by Training Epochs and Vector Size'\n)",
   "id": "f45f11cfdc3fed7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Regression Analysis\n",
    "The code below runs regression analyses on the similarity and analogy results.\n",
    "#### Predictors of Similarity Performance\n",
    "The model coefficient and dot-and-whisker plots show that — not surprisingly — similarity performance is best for recent corpora. The number of training epochs doesn't seem to matter; in fact, the year-by-epochs interaction indicates that more epochs are detrimental to model quality for the earliest corpora. The small positive effects of vector dimensions and the year-by-dimensions interaction show that adding features improves model quality a bit, especially for the most recent corpora."
   ],
   "id": "d6005bce47e4c280"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:28:59.617943Z",
     "start_time": "2025-11-11T02:28:59.382398Z"
    }
   },
   "cell_type": "code",
   "source": "# eval_file already constructed above\nresults = run_regression_analysis(\n    csv_file=str(eval_file),\n    model_type=\"auto\",\n    outcome='similarity_score',\n    predictors=['year', 'vector_size', 'epochs', 'approach'],\n    interactions=[('year', 'vector_size'), ('year', 'epochs'), ('vector_size', 'epochs')],\n)\n\nplot_regression_results(results)",
   "id": "1be5409cb939800c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Predictors of Analogy Performance\n",
    "The regression results for analogy performance confirm what we saw in the plots. Models trained on more recent corpora perform better, and more training epochs contribute to better performance. Although it wasn't immediately apparent in the plots, adding vector dimensions negatively impacts analogy performance, and this effect is not moderated by year."
   ],
   "id": "f7a3c04040374ef7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T02:28:35.120322Z",
     "start_time": "2025-11-11T02:28:34.849010Z"
    }
   },
   "cell_type": "code",
   "source": "# eval_file already constructed above\nresults = run_regression_analysis(\n    csv_file=str(eval_file),\n    model_type=\"auto\",\n    outcome='analogy_score',\n    predictors=['year', 'vector_size', 'epochs', 'approach'],\n    interactions=[('year', 'vector_size'), ('year', 'epochs')],\n)\n\nplot_regression_results(results)",
   "id": "6e2a311be89b518d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Train Final Models**\n",
    "\n",
    "Having explored a range of hyperparameters, we train final models for every year from 1900 through 2019 using what we've learned. A defensible hyperparameter set is:\n",
    "1. `approach=('skip-gram',)`\n",
    "2. `window=(4,)`\n",
    "3. `vector_size=(200,)`\n",
    "4. `epochs=(10,)`\n",
    "5. `min_count=1`"
   ],
   "id": "c28f304453df49ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T21:15:21.012614Z",
     "start_time": "2025-11-11T19:47:51.880474Z"
    }
   },
   "cell_type": "code",
   "source": "build_word2vec_models(\n    ngram_size=size,\n    repo_release_id=release,\n    repo_corpus_id=language,\n    db_path_stub=db_path_stub,\n    dir_suffix='final',\n    years=(1900, 2019),\n    year_step=1,\n    weight_by=('none',),\n    vector_size=(200,),\n    window=(4,),\n    min_count=(1,),\n    approach=('skip-gram',),\n    epochs=(10,),\n    max_parallel_models=33,\n    workers_per_model=3,\n    mode=\"resume\",\n    unk_mode=\"strip\",\n    use_corpus_file=True\n)",
   "id": "ec42ad63345a1e53",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Normalize and Align Models**\n",
    "\n",
    "Before we can use the models for diachronic analysis, we need to unit-normalize the vectors and align them across years using Procrustes rotation. The `normalize_and_align_vectors` function does this."
   ],
   "id": "73e1a2c11f4786f8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-11T22:16:48.915317Z",
     "start_time": "2025-11-11T22:15:23.533418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ngramkit.ngram_train.word2vec.normalize_and_align_models import normalize_and_align_vectors\n",
    "\n",
    "ngram_size = 5\n",
    "proj_dir = '/scratch/edk202/NLP_models/Google_Books/20200217/eng'\n",
    "dir_suffix = 'final'\n",
    "anchor_year = 2000\n",
    "workers = 16\n",
    "\n",
    "normalize_and_align_vectors(\n",
    "    ngram_size=ngram_size,\n",
    "    proj_dir=proj_dir,\n",
    "    dir_suffix=dir_suffix,\n",
    "    anchor_year=anchor_year,\n",
    "    workers=workers\n",
    ")"
   ],
   "id": "eb7b13c0c8c75664",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved normalized anchor model to /scratch/edk202/NLP_models/Google_Books/20200217/eng/5gram_files/models_final/norm_and_align/w2v_y2000_wbnone_vs200_w004_mc001_sg1_e010.kv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing models: 100%|██████████| 119/119 [01:24<00:00,  1.40file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 0:01:25.178946\n",
      "Processed 120 models. Aligned to anchor year 2000.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "db3fa226dcfa74de"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
