{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T02:15:14.380401Z",
     "start_time": "2025-11-17T02:15:13.482791Z"
    }
   },
   "cell_type": "code",
   "source": "%load_ext autoreload\n%autoreload 2\n\nfrom ngramkit.ngram_acquire import download_and_ingest_to_rocksdb\nfrom ngramkit.ngram_filter.pipeline.orchestrator import build_processed_db\nfrom ngramkit.ngram_pivot.pipeline import build_pivoted_db\nfrom ngramkit.utilities.peek import db_head, db_peek, db_peek_prefix",
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure"
  },
  {
   "cell_type": "code",
   "id": "config_cell",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-17T02:15:14.635521Z",
     "start_time": "2025-11-17T02:15:14.394423Z"
    }
   },
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = '/scratch/edk202/NLP_archive/Google_Books/'\n",
    "release = '20200217'\n",
    "language = 'eng-fiction'\n",
    "size = 5"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Step 1: Download and Ingest**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:16:28.953135Z",
     "start_time": "2025-11-16T17:59:55.022610Z"
    }
   },
   "source": [
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=67,\n",
    "    overwrite_db=False,\n",
    "    workers=90,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=False\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-16 12:59:55\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           ...//books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng-fiction/5-\n",
      "DB path:              .../edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams.db\n",
      "File range:           0 to 1448\n",
      "Total files:          1449\n",
      "Files to get:         1449\n",
      "Skipping:             0\n",
      "Download workers:     90\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         False\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████████| 1449/1449 [16:21<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       1449\n",
      "Failed files:                0\n",
      "Total entries written:       203,550,564\n",
      "Write batches flushed:       589\n",
      "Uncompressed data processed: 2.42 TB\n",
      "Processing throughput:       2549.47 MB/sec\n",
      "\n",
      "End Time: 2025-11-16 13:16:28.929272\n",
      "Total Runtime: 0:16:33.886320\n",
      "Time per file: 0:00:00.685912\n",
      "Files per hour: 5248.5\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T19:03:08.716626Z",
     "start_time": "2025-11-16T18:52:03.723741Z"
    }
   },
   "source": [
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=40,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=10.0,\n",
    "    **whitelist_options\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-16 13:52:03\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams.db\n",
      "Target DB:            ...P_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...02/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              40\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...e_Books//20200217/eng-fiction/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 15,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 600 density-based work units...\n",
      "Created 600 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 600 work units with 40 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    4.28M         80.1%         36/40       525·36·39      415.0k/s        10s      \n",
      "    36.17M        83.2%         40/40       445·40·115    1781.4k/s        20s      \n",
      "    63.24M        83.6%         40/40       336·40·224    2084.3k/s        30s      \n",
      "    95.42M        83.2%         40/40       254·40·306    2366.1k/s        40s      \n",
      "   131.68M        82.8%         40/40       163·40·397    2617.3k/s        50s      \n",
      "   164.34M        83.0%         40/40       66·40·494     2724.7k/s       1m00s     \n",
      "   194.58M        82.7%         26/40        0·26·574     2767.9k/s       1m10s     \n",
      "   203.55M        82.8%          0/40        0·0·600      2533.7k/s       1m20s     \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "   203.55M        82.8%          0/40        0·0·600      2272.6k/s       1m29s     \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [05:27<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 48,052,657 items in 328.0s (146,522 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         34.48 GB\n",
      "Compaction completed in 0:00:57\n",
      "Size before:             34.48 GB\n",
      "Size after:              27.71 GB\n",
      "Space saved:             6.77 GB (19.6%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 33,699,321 (estimated)                                                                    │\n",
      "│ Size: 27.71 GB                                                                                   │\n",
      "│ Database: ...202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_processed.db'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T19:11:45.492485Z",
     "start_time": "2025-11-16T19:04:32.824949Z"
    }
   },
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=40,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=10.0,\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:04:32 INFO     root: Logging initialized: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/pivoting_tmp/logs/pivot_pipeline_20251116_140432.log\n",
      "2025-11-16 14:04:32 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: ================================================================================\n",
      "2025-11-16 14:04:32 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: Pivot Pipeline\n",
      "2025-11-16 14:04:32 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: Started: 2025-11-16 14:04:32\n",
      "2025-11-16 14:04:33 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: Source DB: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_processed.db\n",
      "2025-11-16 14:04:33 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: Destination DB: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db\n",
      "2025-11-16 14:04:33 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: Log file: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/pivoting_tmp/logs/pivot_pipeline_20251116_140432.log\n",
      "2025-11-16 14:04:33 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: ================================================================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-16 14:04:33\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...P_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_processed.db\n",
      "Target DB:            ...NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       ...k202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              40\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 600 density-based work units...\n",
      "Created 600 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 600 work units with 40 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     4.50M           47.8x         543·40·17        278.5k/s          16s       \n",
      "     13.69M          48.3x         506·40·54        523.2k/s          26s       \n",
      "     24.28M          48.1x         445·40·115       670.8k/s          36s       \n",
      "     33.34M          46.8x         358·28·214       722.1k/s          46s       \n",
      "     34.84M          46.7x         332·20·248       619.9k/s          56s       \n",
      "     36.49M          46.4x         298·21·281       551.6k/s         1m06s      \n",
      "     40.61M          46.1x         219·40·341       532.9k/s         1m16s      \n",
      "     50.37M          45.4x         99·36·465        584.5k/s         1m26s      \n",
      "     52.66M          45.4x         70·17·513        547.5k/s         1m36s      \n",
      "     52.82M          45.4x         43·20·537        497.4k/s         1m46s      \n",
      "     52.95M          45.4x         18·23·559        455.8k/s         1m56s      \n",
      "     53.04M          45.4x          0·10·590        420.4k/s         2m06s      \n",
      "     53.05M          45.4x          0·1·599         389.6k/s         2m16s      \n",
      "     53.29M          45.5x          0·0·600         364.6k/s         2m26s      \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "     53.29M          45.5x          0·0·600         361.8k/s         2m27s      \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:28<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-16 14:08:51 INFO     ngramkit.ngram_pivot.pipeline.orchestrator: Starting post-ingestion compaction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         16.96 GB\n",
      "Compaction completed in 0:02:52\n",
      "Size before:             16.96 GB\n",
      "Size after:              43.42 GB\n",
      "Space saved:             -26.46 GB (-156.0%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 1,212,469,973 (estimated)                                                                 │\n",
      "│ Size: 43.42 GB                                                                                   │\n",
      "│ Database: ...dk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T19:16:09.109501Z",
     "start_time": "2025-11-16T19:16:08.878839Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1578] <UNK> <UNK> <UNK> <UNK> air\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [1578] <UNK> <UNK> <UNK> <UNK> answer\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1578] <UNK> <UNK> <UNK> <UNK> fleet\n",
      "     Value: 2 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1578] <UNK> <UNK> <UNK> <UNK> man\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1578] <UNK> <UNK> <UNK> <UNK> sake\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_peek",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T19:16:26.119411Z",
     "start_time": "2025-11-16T19:16:25.894686Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2002] attack <UNK> <UNK> world trade\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007d261747461636b203c554e4b3e203c554e4b3e20776f726c64207472616465:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2002] attack <UNK> <UNK> world trade\n",
      "     Value: 152 occurrences in 114 documents\n",
      "\n",
      "[ 2] Key:   [2002] attack <UNK> <UNK> year ago\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [2002] attack <UNK> <UNK> yesterday evening\n",
      "     Value: 3 occurrences in 3 documents\n",
      "\n",
      "[ 4] Key:   [2002] attack <UNK> <UNK> yet <UNK>\n",
      "     Value: 4 occurrences in 4 documents\n",
      "\n",
      "[ 5] Key:   [2002] attack <UNK> <UNK> young age\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "cell_type": "markdown",
   "id": "inspect_prefix_header",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T19:16:30.586409Z",
     "start_time": "2025-11-16T19:16:30.412352Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{size}gram_files/{size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[2011] poor <UNK> happy\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007db706f6f72203c554e4b3e206861707079:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2011] poor <UNK> happy <UNK> <UNK>\n",
      "     Value: 4 occurrences in 4 documents\n",
      "\n",
      "[ 2] Key:   [2011] poor <UNK> happy <UNK> rich\n",
      "     Value: 4 occurrences in 4 documents\n",
      "\n",
      "[ 3] Key:   [2011] poor <UNK> happy <UNK> sad\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 4] Key:   [2011] poor <UNK> happy <UNK> unhappy\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24a850b70859d225",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
