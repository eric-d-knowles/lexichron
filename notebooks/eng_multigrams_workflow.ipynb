{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:29:42.074Z",
     "start_time": "2025-11-30T02:29:42.056701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramkit.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramkit.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramkit.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramkit.utilities.peek import db_head, db_peek, db_peek_prefix"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T02:29:45.510775Z",
     "start_time": "2025-11-30T02:29:45.503826Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = '/scratch/edk202/NLP_archive/Google_Books/'\n",
    "release = '20200217'\n",
    "language = 'eng'\n",
    "ngram_size = 5\n",
    "bin_size = 5"
   ],
   "id": "d5328f85c059eda4",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Step 1: Download and Ingest**",
   "id": "2996606b00401532"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {},
   "source": [
    "combined_bigrams_download = {\"lower class\", \"working class\", \"middle class\", \"upper class\", \"human being\"}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=98,\n",
    "    overwrite_db=False,\n",
    "    workers=80,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:35:48.695098Z",
     "start_time": "2025-11-30T02:32:05.320348Z"
    }
   },
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\"lower-class\", \"working-class\", \"middle-class\", \"upper-class\", \"human-being\"}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-29 21:32:05\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             5 (5-year bins)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ra/Google_Books//20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    11.50M        79.8%         36/36       539·36·25      379.6k/s        30s      \n",
      "    55.19M        77.4%         36/36       511·36·53      915.8k/s       1m00s     \n",
      "   100.98M        78.5%         36/36       489·36·75     1118.4k/s       1m30s     \n",
      "   142.59M        78.7%         36/36       474·36·90     1185.5k/s       2m00s     \n",
      "   186.59M        79.2%         36/36       466·36·98     1241.5k/s       2m30s     \n",
      "   232.14M        79.7%         36/36       453·36·111    1287.9k/s       3m00s     \n",
      "   277.15M        80.3%         36/36       446·36·118    1318.1k/s       3m30s     \n",
      "   317.59M        80.8%         36/36       439·36·125    1321.6k/s       4m00s     \n",
      "   366.97M        81.3%         36/36       437·36·127    1357.7k/s       4m30s     \n",
      "   412.93M        81.6%         36/36       435·36·129    1375.1k/s       5m00s     \n",
      "   453.81M        81.8%         36/36       426·36·138    1373.9k/s       5m30s     \n",
      "   499.95M        82.2%         36/36       419·36·145    1387.7k/s       6m00s     \n",
      "   540.73M        82.6%         36/36       410·36·154    1385.5k/s       6m30s     \n",
      "   586.65M        82.8%         36/36       397·36·167    1395.8k/s       7m00s     \n",
      "   628.05M        82.8%         36/36       378·36·186    1394.9k/s       7m30s     \n",
      "   670.53M        82.7%         36/36       367·36·197    1396.1k/s       8m00s     \n",
      "   715.24M        82.6%         36/36       353·36·211    1401.6k/s       8m30s     \n",
      "   758.37M        82.7%         36/36       341·36·223    1403.6k/s       9m00s     \n",
      "   796.28M        82.7%         36/36       323·36·241    1396.4k/s       9m30s     \n",
      "   840.36M        82.7%         36/36       316·36·248    1400.0k/s       10m00s    \n",
      "   887.66M        82.6%         36/36       312·36·252    1408.3k/s       10m30s    \n",
      "   930.80M        82.6%         36/36       300·36·264    1409.7k/s       11m00s    \n",
      "   973.24M        82.6%         36/36       286·36·278    1409.9k/s       11m30s    \n",
      "    1.02B         82.7%         36/36       279·36·285    1412.9k/s       12m00s    \n",
      "    1.06B         82.8%         36/36       269·36·295    1411.0k/s       12m30s    \n",
      "    1.10B         82.9%         36/36       256·36·308    1410.7k/s       13m00s    \n",
      "    1.14B         83.1%         36/36       246·36·318    1411.1k/s       13m30s    \n",
      "    1.18B         83.2%         36/36       240·36·324    1409.5k/s       14m00s    \n",
      "    1.23B         83.4%         36/36       237·36·327    1411.4k/s       14m30s    \n",
      "    1.27B         83.5%         36/36       233·36·331    1413.8k/s       15m00s    \n",
      "    1.31B         83.7%         36/36       229·36·335    1410.6k/s       15m30s    \n",
      "    1.36B         83.8%         36/36       227·36·337    1414.1k/s       16m00s    \n",
      "    1.40B         83.9%         36/36       217·36·347    1413.5k/s       16m30s    \n",
      "    1.44B         83.9%         36/36       206·36·358    1415.2k/s       17m00s    \n",
      "    1.48B         84.0%         36/36       197·36·367    1413.6k/s       17m30s    \n",
      "    1.52B         84.1%         36/36       185·36·379    1411.6k/s       18m00s    \n",
      "    1.57B         84.0%         36/36       178·36·386    1416.0k/s       18m30s    \n",
      "    1.61B         84.0%         36/36       162·36·402    1413.9k/s       19m00s    \n",
      "    1.66B         84.1%         36/36       153·36·411    1414.8k/s       19m30s    \n",
      "    1.70B         84.0%         36/36       137·36·427    1415.1k/s       20m00s    \n",
      "    1.74B         83.9%         36/36       123·36·441    1416.9k/s       20m30s    \n",
      "    1.79B         84.0%         36/36       112·36·452    1416.6k/s       21m00s    \n",
      "    1.83B         83.9%         36/36       101·36·463    1418.9k/s       21m30s    \n",
      "    1.87B         84.0%         36/36       88·36·476     1418.6k/s       22m00s    \n",
      "    1.91B         84.0%         36/36       77·36·487     1417.9k/s       22m30s    \n",
      "    1.96B         83.9%         36/36       62·36·502     1420.2k/s       23m00s    \n",
      "    2.00B         83.9%         36/36       52·36·512     1420.3k/s       23m30s    \n",
      "    2.05B         83.9%         36/36       42·36·522     1420.7k/s       24m00s    \n",
      "    2.09B         83.9%         36/36       30·36·534     1422.5k/s       24m30s    \n",
      "    2.14B         83.9%         36/36       25·36·539     1423.1k/s       25m00s    \n",
      "    2.18B         83.9%         36/36       16·36·548     1424.0k/s       25m30s    \n",
      "    2.22B         83.9%         36/36        5·36·559     1426.0k/s       26m00s    \n",
      "    2.27B         83.9%         32/36        0·32·568     1425.8k/s       26m30s    \n",
      "    2.31B         83.9%         27/36        0·27·573     1424.8k/s       27m00s    \n",
      "    2.34B         83.9%         27/36        0·27·573     1419.7k/s       27m30s    \n",
      "    2.38B         83.9%         24/36        0·24·576     1414.1k/s       28m00s    \n",
      "    2.41B         83.9%         20/36        0·20·580     1407.7k/s       28m30s    \n",
      "    2.43B         84.0%         15/36        0·15·585     1396.9k/s       29m00s    \n",
      "    2.45B         84.0%          9/36        0·9·591      1382.9k/s       29m30s    \n",
      "    2.46B         84.0%          5/36        0·5·595      1366.1k/s       30m00s    \n",
      "    2.47B         84.0%          4/36        0·4·596      1347.3k/s       30m30s    \n",
      "    2.47B         84.0%          2/36        0·2·598      1327.6k/s       31m00s    \n",
      "    2.47B         84.0%          2/36        0·2·598      1308.2k/s       31m30s    \n",
      "    2.48B         84.1%          1/36        0·1·599      1289.2k/s       32m00s    \n",
      "    2.48B         84.1%          1/36        0·1·599      1270.1k/s       32m30s    \n",
      "    2.48B         84.1%          1/36        0·1·599      1251.3k/s       33m00s    \n",
      "    2.48B         84.1%          1/36        0·1·599      1233.4k/s       33m30s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    2.48B         84.1%          0/36        0·0·600      1215.8k/s       33m59s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [25:26<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 499,921,360 items in 1526.6s (327,480 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         180.91 GB\n",
      "Compaction completed in 0:04:14\n",
      "Size before:             180.91 GB\n",
      "Size after:              115.47 GB\n",
      "Space saved:             65.44 GB (36.2%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 437,313,451 (estimated)                                                                   │\n",
      "│ Size: 115.47 GB                                                                                  │\n",
      "│ Database: ...atch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T03:53:48.075028Z",
     "start_time": "2025-11-30T03:35:48.701865Z"
    }
   },
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=40,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-29 22:35:49\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              40\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 40 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     33.40M           0.0x          560·40·0       1019.1k/s          33s       \n",
      "    115.95M          13.8x         520·40·40       1846.5k/s         1m02s      \n",
      "    166.61M          17.0x         491·40·69       1795.1k/s         1m32s      \n",
      "    228.65M          17.7x         448·40·112      1862.0k/s         2m02s      \n",
      "    301.98M          17.7x         381·40·179      1976.6k/s         2m32s      \n",
      "    375.45M          17.4x         265·40·295      2053.9k/s         3m02s      \n",
      "    432.82M          17.5x         208·40·352      2034.2k/s         3m32s      \n",
      "    502.37M          17.5x         127·40·433      2069.1k/s         4m02s      \n",
      "    552.85M          17.6x          5·16·579       2026.8k/s         4m32s      \n",
      "    556.06M          17.5x          0·1·599        1836.6k/s         5m02s      \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    557.46M          17.6x          0·0·600        1710.5k/s         5m25s      \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:29<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         72.75 GB\n",
      "Compaction completed in 0:11:56\n",
      "Size before:             72.75 GB\n",
      "Size after:              172.43 GB\n",
      "Space saved:             -99.68 GB (-137.0%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 4,896,116,215 (estimated)                                                                 │\n",
      "│ Size: 172.43 GB                                                                                  │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:02:50.775864Z",
     "start_time": "2025-11-30T05:02:49.829266Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1470] <UNK> <UNK> <UNK> <UNK> absence\n",
      "     Value: 5 occurrences in 3 documents\n",
      "\n",
      "[ 2] Key:   [1470] <UNK> <UNK> <UNK> <UNK> accordance\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1470] <UNK> <UNK> <UNK> <UNK> accordingly\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1470] <UNK> <UNK> <UNK> <UNK> accrue\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1470] <UNK> <UNK> <UNK> <UNK> acre\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_peek",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:02:57.074588Z",
     "start_time": "2025-11-30T05:02:56.264112Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2015] working-class <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007df776f726b696e672d636c617373203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2015] working-class <UNK> <UNK> ability\n",
      "     Value: 13 occurrences in 13 documents\n",
      "\n",
      "[ 2] Key:   [2015] working-class <UNK> <UNK> able\n",
      "     Value: 120 occurrences in 117 documents\n",
      "\n",
      "[ 3] Key:   [2015] working-class <UNK> <UNK> abolition\n",
      "     Value: 45 occurrences in 42 documents\n",
      "\n",
      "[ 4] Key:   [2015] working-class <UNK> <UNK> abortive\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 5] Key:   [2015] working-class <UNK> <UNK> absence\n",
      "     Value: 13 occurrences in 13 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "inspect_prefix_header",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T05:03:07.570548Z",
     "start_time": "2025-11-30T05:03:06.782373Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[2000] <UNK> working-class\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007d03c554e4b3e20776f726b696e672d636c617373:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2000] <UNK> working-class <UNK>\n",
      "     Value: 47 occurrences in 46 documents\n",
      "\n",
      "[ 2] Key:   [2000] <UNK> working-class <UNK> <UNK>\n",
      "     Value: 138,757 occurrences in 128,235 documents\n",
      "\n",
      "[ 3] Key:   [2000] <UNK> working-class <UNK> ability\n",
      "     Value: 3 occurrences in 3 documents\n",
      "\n",
      "[ 4] Key:   [2000] <UNK> working-class <UNK> able\n",
      "     Value: 108 occurrences in 107 documents\n",
      "\n",
      "[ 5] Key:   [2000] <UNK> working-class <UNK> absent\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24a850b70859d225",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
