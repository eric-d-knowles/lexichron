{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a1e3e9b344be",
   "metadata": {},
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd936c0a392494",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f261bd4a6317873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.700012Z",
     "start_time": "2026-01-02T00:15:41.111800Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramprep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramprep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramprep.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramprep.utilities.peek import db_head, db_peek, db_peek_prefix\n",
    "from ngramprep.utilities.count_items import count_db_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5328f85c059eda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.870970Z",
     "start_time": "2026-01-02T00:15:41.713856Z"
    }
   },
   "outputs": [],
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = None\n",
    "release = '20200217'\n",
    "language = 'eng'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996606b00401532",
   "metadata": {},
   "source": [
    "## **Step 1: Download and Ingest**\n",
    "\n",
    "Specifying `combined_bigrams_download` will convert compound terms into single, hyphenated tokens. This process is case-sensitive, so we specify all common capitalization patterns.\n",
    "\n",
    "If you're resuming downloads after an interruption, there may be a lag before you see any output. The RocksDB is being repaired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:50:58.847936Z",
     "start_time": "2025-12-24T20:09:09.627237Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_bigrams_download = {\n",
    "    \"human being\", \"Human being\", \"Human Being\", \"human beings\", \"Human beings\", \"Human Beings\"\n",
    "}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=923,\n",
    "    overwrite_db=False,\n",
    "    workers=128,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams.\n",
    "\n",
    "`always_include_tokens` is applied after case-normalization, so we use all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:13:21.359494Z",
     "start_time": "2025-12-25T05:08:45.236033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-13 02:46:15\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       8\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Filter Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Whitelist mode:       ENABLED\n",
      "(All other filters are bypassed when whitelist is active)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ra/Google_Books//20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/lexichron/src/ngramprep/__init__.py\", line 39, in <module>\n",
      "    _ensure_spacy_models()\n",
      "  File \"/scratch/edk202/lexichron/src/ngramprep/__init__.py\", line 34, in _ensure_spacy_models\n",
      "    spacy.load(model)\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/__init__.py\", line 52, in load\n",
      "    return util.load_model(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 524, in load_model\n",
      "    return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 560, in load_model_from_package\n",
      "    return cls.load(vocab=vocab, disable=disable, enable=enable, exclude=exclude, config=config)  # type: ignore[attr-defined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/fr_core_news_sm/__init__.py\", line 10, in load\n",
      "    return load_model_from_init_py(__file__, **overrides)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 741, in load_model_from_init_py\n",
      "    return load_model_from_path(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 598, in load_model_from_path\n",
      "    nlp = load_model_from_config(\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 645, in load_model_from_config\n",
      "    lang_cls = get_lang_class(nlp_config[\"lang\"])\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 424, in get_lang_class\n",
      "    module = importlib.import_module(f\".lang.{lang}\", \"spacy\")\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/lang/fr/__init__.py\", line 11, in <module>\n",
      "    from .tokenizer_exceptions import TOKEN_MATCH, TOKENIZER_EXCEPTIONS\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/lang/fr/tokenizer_exceptions.py\", line 441, in <module>\n",
      "    TOKEN_MATCH = re.compile(\n",
      "                  ^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/__init__.py\", line 227, in compile\n",
      "    return _compile(pattern, flags)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/__init__.py\", line 294, in _compile\n",
      "    p = _compiler.compile(pattern, flags)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/_compiler.py\", line 745, in compile\n",
      "    p = _parser.parse(p, flags)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/_parser.py\", line 989, in parse\n",
      "    p = _parse_sub(source, state, flags & SRE_FLAG_VERBOSE, 0)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/_parser.py\", line 464, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/_parser.py\", line 872, in _parse\n",
      "    p = _parse_sub(source, state, sub_verbose, nested + 1)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/_parser.py\", line 464, in _parse_sub\n",
      "    itemsappend(_parse(source, state, verbose, nested + 1,\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/_parser.py\", line 577, in _parse\n",
      "    code1 = _class_escape(source, this)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/re/_parser.py\", line 337, in _class_escape\n",
      "    return LITERAL, int(escape[2:], 16)\n",
      "                    ^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/lexichron/src/ngramprep/__init__.py\", line 39, in <module>\n",
      "    _ensure_spacy_models()\n",
      "  File \"/scratch/edk202/lexichron/src/ngramprep/__init__.py\", line 34, in _ensure_spacy_models\n",
      "    spacy.load(model)\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/__init__.py\", line 52, in load\n",
      "    return util.load_model(\n",
      "           ^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 524, in load_model\n",
      "    return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 560, in load_model_from_package\n",
      "    return cls.load(vocab=vocab, disable=disable, enable=enable, exclude=exclude, config=config)  # type: ignore[attr-defined]\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/zh_core_web_sm/__init__.py\", line 10, in load\n",
      "    return load_model_from_init_py(__file__, **overrides)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 741, in load_model_from_init_py\n",
      "    return load_model_from_path(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 606, in load_model_from_path\n",
      "    return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/language.py\", line 2245, in from_disk\n",
      "    util.from_disk(path, deserializers, exclude)  # type: ignore[arg-type]\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 1448, in from_disk\n",
      "    reader(path / key)\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/language.py\", line 2231, in <lambda>\n",
      "    deserializers[\"tokenizer\"] = lambda p: self.tokenizer.from_disk(  # type: ignore[union-attr]\n",
      "                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/lang/zh/__init__.py\", line 282, in from_disk\n",
      "    util.from_disk(path, serializers, [])\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/util.py\", line 1448, in from_disk\n",
      "    reader(path / key)\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/lang/zh/__init__.py\", line 279, in <lambda>\n",
      "    \"pkuseg_model\": lambda p: load_pkuseg_model(p),\n",
      "                              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy/lang/zh/__init__.py\", line 261, in load_pkuseg_model\n",
      "    self.pkuseg_seg = spacy_pkuseg.pkuseg(path)\n",
      "                      ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/spacy_pkuseg/__init__.py\", line 234, in __init__\n",
      "    self.feature_extractor = FeatureExtractor.load()\n",
      "                             ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"spacy_pkuseg/feature_extractor.pyx\", line 593, in spacy_pkuseg.feature_extractor.FeatureExtractor.load\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/srsly/_msgpack_api.py\", line 55, in read_msgpack\n",
      "    msg = msgpack.load(f, raw=False, use_list=use_list)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/srsly/msgpack/__init__.py\", line 73, in unpack\n",
      "    return _unpackb(data, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"srsly/msgpack/_unpacker.pyx\", line 206, in srsly.msgpack._unpacker.unpackb\n",
      "  File \"/scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages/srsly/msgpack/_msgpack_numpy.py\", line 65, in decode_numpy\n",
      "    def decode_numpy(obj, chain=None):\n",
      "    \n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m      5\u001b[39m whitelist_options = {\n\u001b[32m      6\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mwhitelist_path\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdb_path_stub\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrelease\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlanguage\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/1gram_files/1grams_processed.db/whitelist.txt\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33moutput_whitelist_path\u001b[39m\u001b[33m'\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m      8\u001b[39m }\n\u001b[32m     10\u001b[39m always_include_tokens = {\n\u001b[32m     11\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhe\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mshe\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mhim\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mher\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     25\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mbrothers\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33msisters\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     26\u001b[39m }\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[43mbuild_processed_db\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrestart\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m    \u001b[49m\u001b[43mngram_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mngram_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_release_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelease\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrepo_corpus_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdb_path_stub\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdb_path_stub\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m36\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_initial_work_units\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mwork_unit_claim_order\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrandom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_partitions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cached_partitions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprogress_every_s\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30.0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43malways_include\u001b[49m\u001b[43m=\u001b[49m\u001b[43malways_include_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompact_after_ingest\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfilter_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mwhitelist_options\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m)\u001b[49m;\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/lexichron/src/ngramprep/ngram_filter/pipeline/orchestrator.py:837\u001b[39m, in \u001b[36mbuild_processed_db\u001b[39m\u001b[34m(filter_config, pipeline_config, ngram_size, repo_release_id, repo_corpus_id, db_path_stub, stop_set, lemma_gen, min_len, whitelist_path, whitelist_min_count, whitelist_top_n, always_include, bin_size, min_context_tokens, num_workers, mode, use_smart_partitioning, num_initial_work_units, cache_partitions, use_cached_partitions, samples_per_worker, work_unit_claim_order, flush_interval_s, progress_every_s, ingest_num_readers, ingest_batch_items, ingest_queue_size, compact_after_ingest, output_whitelist_path, output_whitelist_top_n, output_whitelist_year_range, output_whitelist_spell_check, output_whitelist_spell_check_language)\u001b[39m\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[32m    836\u001b[39m orchestrator = PipelineOrchestrator(constructed_pipeline_config, filter_config)\n\u001b[32m--> \u001b[39m\u001b[32m837\u001b[39m \u001b[43morchestrator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(dst_db)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/lexichron/src/ngramprep/ngram_filter/pipeline/orchestrator.py:108\u001b[39m, in \u001b[36mPipelineOrchestrator.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Execute pipeline phases\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28mself\u001b[39m._create_work_units()\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_work_units\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[38;5;28mself\u001b[39m._ingest_shards()\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._finalize_database()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/lexichron/src/ngramprep/ngram_filter/pipeline/orchestrator.py:301\u001b[39m, in \u001b[36mPipelineOrchestrator._process_work_units\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    297\u001b[39m progress_reporter = \u001b[38;5;28mself\u001b[39m._setup_progress_monitoring()\n\u001b[32m    299\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    300\u001b[39m     \u001b[38;5;66;03m# Run the worker pool (workers write shards, no concurrent ingestion)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m     \u001b[43mrun_worker_pool_simple\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m        \u001b[49m\u001b[43msrc_db_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemp_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43msrc_db\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwork_tracker_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemp_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mwork_tracker\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtemp_paths\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43moutput_dir\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilter_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpipeline_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    308\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworker_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_worker_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    309\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcounters\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    310\u001b[39m \u001b[43m            \u001b[49m\u001b[43mprogress_reporter\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcounters\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    311\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprogress_reporter\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    316\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m─\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m37\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m final \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m─\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39m\u001b[32m37\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    318\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m     \u001b[38;5;66;03m# Stop progress reporter\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/lexichron/src/ngramprep/ngram_filter/pipeline/simple_worker_pool.py:66\u001b[39m, in \u001b[36mrun_worker_pool_simple\u001b[39m\u001b[34m(num_workers, src_db_path, work_tracker_path, output_dir, filter_config, pipeline_config, worker_config, counters)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m worker_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_workers):\n\u001b[32m     51\u001b[39m     process = ctx.Process(\n\u001b[32m     52\u001b[39m         target=worker_process,\n\u001b[32m     53\u001b[39m         args=(\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m         name=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mngf:worker-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mworker_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m     processes.append(process)\n\u001b[32m     69\u001b[39m \u001b[38;5;66;03m# Wait for all workers to complete\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/process.py:121\u001b[39m, in \u001b[36mBaseProcess.start\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process._config.get(\u001b[33m'\u001b[39m\u001b[33mdaemon\u001b[39m\u001b[33m'\u001b[39m), \\\n\u001b[32m    119\u001b[39m        \u001b[33m'\u001b[39m\u001b[33mdaemonic processes are not allowed to have children\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    120\u001b[39m _cleanup()\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28mself\u001b[39m._popen = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[38;5;28mself\u001b[39m._sentinel = \u001b[38;5;28mself\u001b[39m._popen.sentinel\n\u001b[32m    123\u001b[39m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[32m    124\u001b[39m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/context.py:288\u001b[39m, in \u001b[36mSpawnProcess._Popen\u001b[39m\u001b[34m(process_obj)\u001b[39m\n\u001b[32m    285\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_Popen\u001b[39m(process_obj):\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/popen_spawn_posix.py:32\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[32m     31\u001b[39m     \u001b[38;5;28mself\u001b[39m._fds = []\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/popen_fork.py:19\u001b[39m, in \u001b[36mPopen.__init__\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mself\u001b[39m.returncode = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.finalizer = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/scratch/edk202/.conda/envs/lexichron/lib/python3.11/multiprocessing/popen_spawn_posix.py:62\u001b[39m, in \u001b[36mPopen._launch\u001b[39m\u001b[34m(self, process_obj)\u001b[39m\n\u001b[32m     60\u001b[39m     \u001b[38;5;28mself\u001b[39m.sentinel = parent_r\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[33m'\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m'\u001b[39m, closefd=\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m---> \u001b[39m\u001b[32m62\u001b[39m         f.write(fp.getbuffer())\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     64\u001b[39m     fds_to_close = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    'he', 'she',\n",
    "    'him', 'her',\n",
    "    'his', 'hers',\n",
    "    'himself', 'herself',\n",
    "    'man', 'woman',\n",
    "    'men', 'women',\n",
    "    'male', 'female',\n",
    "    'boy', 'girl',\n",
    "    'boys', 'girls',\n",
    "    'father', 'mother',\n",
    "    'fathers', 'mothers',\n",
    "    'son', 'daughter',\n",
    "    'sons', 'daughters',\n",
    "    'brother', 'sister',\n",
    "    'brothers', 'sisters'\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    compact_after_ingest=True,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "## **Step 3: Pivot to Yearly Indices**\n",
    "This function rearranges the filtered data such that each record has a year (or year bin) prefix. Ideal for time-series announcement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:34:30.978547Z",
     "start_time": "2025-12-25T06:16:25.748406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-12 21:50:30\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 600 density-based work units...\n",
      "Created 600 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 600 work units with 30 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     26.21M           0.0x          570·30·0        612.3k/s          43s       \n",
      "     31.91M           0.0x          570·30·0        438.3k/s         1m12s      \n",
      "     31.91M           0.0x          570·30·0        310.4k/s         1m42s      \n",
      "     34.73M          32.1x         560·30·10        261.5k/s         2m12s      \n",
      "     54.19M          33.9x         540·30·30        332.9k/s         2m42s      \n",
      "     65.82M          27.9x         540·30·30        341.4k/s         3m12s      \n",
      "     66.63M          30.1x         540·30·30        299.1k/s         3m42s      \n",
      "     73.36M          40.6x         524·30·46        290.2k/s         4m12s      \n",
      "     89.14M          40.5x         513·30·57        315.2k/s         4m42s      \n",
      "     98.30M          37.4x         510·30·60        314.2k/s         5m12s      \n",
      "     99.99M          40.3x         508·30·62        291.7k/s         5m42s      \n",
      "    110.28M          45.4x         493·30·77        295.8k/s         6m12s      \n",
      "    118.78M          47.2x         482·30·88        294.9k/s         6m42s      \n",
      "    127.14M          46.3x         466·30·104       293.7k/s         7m12s      \n",
      "    135.36M          49.3x         448·30·122       292.5k/s         7m42s      \n",
      "    145.72M          48.9x         430·30·140       295.7k/s         8m12s      \n",
      "    157.95M          48.2x         408·30·162       302.1k/s         8m42s      \n",
      "    167.61M          49.0x         385·30·185       303.2k/s         9m12s      \n",
      "    178.04M          49.2x         354·30·216       305.5k/s         9m42s      \n",
      "    188.03M          49.1x         326·30·244       306.8k/s         10m12s     \n",
      "    198.14M          49.4x         296·30·274       308.2k/s         10m42s     \n",
      "    211.19M          47.6x         276·30·294       313.9k/s         11m12s     \n",
      "    221.72M          47.6x         258·30·312       315.5k/s         11m42s     \n",
      "    230.85M          47.6x         244·30·326       315.0k/s         12m12s     \n",
      "    242.67M          47.5x         218·30·352       318.1k/s         12m42s     \n",
      "    253.62M          47.3x         199·30·371       319.9k/s         13m12s     \n",
      "    262.24M          47.5x         181·30·389       318.7k/s         13m42s     \n",
      "    273.44M          47.3x         158·30·412       320.6k/s         14m12s     \n",
      "    284.64M          47.4x         131·30·439       322.4k/s         14m42s     \n",
      "    294.27M          47.5x         107·30·463       322.4k/s         15m12s     \n",
      "    299.61M          48.0x          0·13·587        317.8k/s         15m42s     \n",
      "    301.25M          48.4x          0·1·599         309.7k/s         16m12s     \n",
      "    301.25M          48.4x          0·1·599         300.4k/s         16m42s     \n",
      "    301.25M          48.4x          0·1·599         291.7k/s         17m12s     \n",
      "    301.40M          48.6x          0·1·599         283.6k/s         17m42s     \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    301.40M          48.6x          0·0·600         280.4k/s         17m54s     \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:44<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         211.29 GB\n",
      "Compaction completed in 0:44:59\n",
      "Size before:             211.29 GB\n",
      "Size after:              517.97 GB\n",
      "Space saved:             -306.68 GB (-145.1%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 14,661,317,082 (estimated)                                                                │\n",
      "│ Size: 517.97 GB                                                                                  │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=False,\n",
    "    compact_after_ingest=True,\n",
    "    progress_every_s=30.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "## **Optional: Inspect Database Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "### `db_head`: Show first N records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:35:13.418972Z",
     "start_time": "2025-12-25T06:35:03.313440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   <UNK> <UNK> <UNK> ab ab\n",
      "     Value: Total: 48 occurrences in 41 volumes (1965-2019, 17 bins)\n",
      "\n",
      "[ 2] Key:   <UNK> <UNK> <UNK> ab abbreviation\n",
      "     Value: Total: 47 occurrences in 47 volumes (1998-2001, 3 bins)\n",
      "\n",
      "[ 3] Key:   <UNK> <UNK> <UNK> ab abstract\n",
      "     Value: Total: 43 occurrences in 42 volumes (1957-2017, 19 bins)\n",
      "\n",
      "[ 4] Key:   <UNK> <UNK> <UNK> ab ad\n",
      "     Value: Total: 41 occurrences in 41 volumes (1939-2019, 18 bins)\n",
      "\n",
      "[ 5] Key:   <UNK> <UNK> <UNK> ab albino\n",
      "     Value: Total: 76 occurrences in 69 volumes (1832-2005, 30 bins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "db_head(db, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "### `db_peek`: Show Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3519e6ee50c4109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:06.913551Z",
     "start_time": "2025-12-25T06:39:56.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from c3a0203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "db_peek(db, start_key=\"à <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63e70a0ce04fb",
   "metadata": {},
   "source": [
    "### `db_peek_prefix`: Show records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:20.607162Z",
     "start_time": "2025-12-25T06:40:10.425620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 key-value pairs with prefix 62:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   ba <UNK> <UNK> <UNK> ab\n",
      "     Value: Total: 137 occurrences in 94 volumes (1908-2019, 34 bins)\n",
      "\n",
      "[ 2] Key:   ba <UNK> <UNK> <UNK> absent\n",
      "     Value: Total: 52 occurrences in 49 volumes (1861-2008, 33 bins)\n",
      "\n",
      "[ 3] Key:   ba <UNK> <UNK> <UNK> acid\n",
      "     Value: Total: 234 occurrences in 207 volumes (1963-2017, 46 bins)\n",
      "\n",
      "[ 4] Key:   ba <UNK> <UNK> <UNK> ack\n",
      "     Value: Total: 235 occurrences in 219 volumes (1851-2019, 55 bins)\n",
      "\n",
      "[ 5] Key:   ba <UNK> <UNK> <UNK> act\n",
      "     Value: Total: 251 occurrences in 169 volumes (1924-2019, 54 bins)\n",
      "\n",
      "[ 6] Key:   ba <UNK> <UNK> <UNK> activity\n",
      "     Value: Total: 67 occurrences in 43 volumes (1967-2012, 11 bins)\n",
      "\n",
      "[ 7] Key:   ba <UNK> <UNK> <UNK> ad\n",
      "     Value: Total: 640 occurrences in 598 volumes (1781-2019, 119 bins)\n",
      "\n",
      "[ 8] Key:   ba <UNK> <UNK> <UNK> administrative\n",
      "     Value: Total: 84 occurrences in 46 volumes (1994-2008, 10 bins)\n",
      "\n",
      "[ 9] Key:   ba <UNK> <UNK> <UNK> ah\n",
      "     Value: Total: 373 occurrences in 333 volumes (1849-2019, 97 bins)\n",
      "\n",
      "[10] Key:   ba <UNK> <UNK> <UNK> air\n",
      "     Value: Total: 128 occurrences in 47 volumes (1982-1991, 7 bins)\n",
      "\n",
      "[11] Key:   ba <UNK> <UNK> <UNK> al\n",
      "     Value: Total: 1,974 occurrences in 984 volumes (1889-2019, 98 bins)\n",
      "\n",
      "[12] Key:   ba <UNK> <UNK> <UNK> also\n",
      "     Value: Total: 45 occurrences in 45 volumes (1899-2018, 26 bins)\n",
      "\n",
      "[13] Key:   ba <UNK> <UNK> <UNK> among\n",
      "     Value: Total: 115 occurrences in 115 volumes (2018-2018, 1 bins)\n",
      "\n",
      "[14] Key:   ba <UNK> <UNK> <UNK> analysis\n",
      "     Value: Total: 98 occurrences in 41 volumes (1982-1991, 7 bins)\n",
      "\n",
      "[15] Key:   ba <UNK> <UNK> <UNK> ancient\n",
      "     Value: Total: 49 occurrences in 41 volumes (2006-2019, 13 bins)\n",
      "\n",
      "[16] Key:   ba <UNK> <UNK> <UNK> angle\n",
      "     Value: Total: 112 occurrences in 84 volumes (1781-2019, 51 bins)\n",
      "\n",
      "[17] Key:   ba <UNK> <UNK> <UNK> animation\n",
      "     Value: Total: 154 occurrences in 53 volumes (1989-2019, 25 bins)\n",
      "\n",
      "[18] Key:   ba <UNK> <UNK> <UNK> annual\n",
      "     Value: Total: 92 occurrences in 86 volumes (1941-1998, 33 bins)\n",
      "\n",
      "[19] Key:   ba <UNK> <UNK> <UNK> anterior\n",
      "     Value: Total: 61 occurrences in 52 volumes (1997-2013, 15 bins)\n",
      "\n",
      "[20] Key:   ba <UNK> <UNK> <UNK> apparent\n",
      "     Value: Total: 136 occurrences in 103 volumes (1968-2017, 24 bins)\n",
      "\n",
      "[21] Key:   ba <UNK> <UNK> <UNK> application\n",
      "     Value: Total: 172 occurrences in 120 volumes (1980-1991, 11 bins)\n",
      "\n",
      "[22] Key:   ba <UNK> <UNK> <UNK> appropriate\n",
      "     Value: Total: 47 occurrences in 40 volumes (1974-1993, 13 bins)\n",
      "\n",
      "[23] Key:   ba <UNK> <UNK> <UNK> appropriation\n",
      "     Value: Total: 43 occurrences in 41 volumes (1862-1967, 30 bins)\n",
      "\n",
      "[24] Key:   ba <UNK> <UNK> <UNK> arch\n",
      "     Value: Total: 74 occurrences in 59 volumes (1962-2016, 31 bins)\n",
      "\n",
      "[25] Key:   ba <UNK> <UNK> <UNK> architecture\n",
      "     Value: Total: 241 occurrences in 115 volumes (1965-2019, 32 bins)\n",
      "\n",
      "[26] Key:   ba <UNK> <UNK> <UNK> arm\n",
      "     Value: Total: 70 occurrences in 70 volumes (1914-2017, 35 bins)\n",
      "\n",
      "[27] Key:   ba <UNK> <UNK> <UNK> art\n",
      "     Value: Total: 294 occurrences in 179 volumes (1975-2019, 41 bins)\n",
      "\n",
      "[28] Key:   ba <UNK> <UNK> <UNK> assistant\n",
      "     Value: Total: 181 occurrences in 91 volumes (1965-2014, 32 bins)\n",
      "\n",
      "[29] Key:   ba <UNK> <UNK> <UNK> associate\n",
      "     Value: Total: 437 occurrences in 56 volumes (1984-2008, 21 bins)\n",
      "\n",
      "[30] Key:   ba <UNK> <UNK> <UNK> attorney\n",
      "     Value: Total: 88 occurrences in 53 volumes (1978-2010, 15 bins)\n",
      "\n",
      "[31] Key:   ba <UNK> <UNK> <UNK> authority\n",
      "     Value: Total: 411 occurrences in 357 volumes (1987-2018, 31 bins)\n",
      "\n",
      "[32] Key:   ba <UNK> <UNK> <UNK> aye\n",
      "     Value: Total: 221 occurrences in 220 volumes (1831-2019, 91 bins)\n",
      "\n",
      "[33] Key:   ba <UNK> <UNK> <UNK> ba\n",
      "     Value: Total: 4,144 occurrences in 2,788 volumes (1781-2019, 177 bins)\n",
      "\n",
      "[34] Key:   ba <UNK> <UNK> <UNK> bachelor\n",
      "     Value: Total: 144 occurrences in 144 volumes (1960-2016, 38 bins)\n",
      "\n",
      "[35] Key:   ba <UNK> <UNK> <UNK> ball\n",
      "     Value: Total: 84 occurrences in 76 volumes (1939-2018, 41 bins)\n",
      "\n",
      "[36] Key:   ba <UNK> <UNK> <UNK> bar\n",
      "     Value: Total: 47 occurrences in 47 volumes (1953-1996, 17 bins)\n",
      "\n",
      "[37] Key:   ba <UNK> <UNK> <UNK> barrister\n",
      "     Value: Total: 205 occurrences in 129 volumes (1931-2015, 45 bins)\n",
      "\n",
      "[38] Key:   ba <UNK> <UNK> <UNK> base\n",
      "     Value: Total: 79 occurrences in 79 volumes (1769-2004, 53 bins)\n",
      "\n",
      "[39] Key:   ba <UNK> <UNK> <UNK> bc\n",
      "     Value: Total: 1,564 occurrences in 1,341 volumes (1743-2019, 164 bins)\n",
      "\n",
      "[40] Key:   ba <UNK> <UNK> <UNK> bed\n",
      "     Value: Total: 870 occurrences in 462 volumes (1958-2019, 59 bins)\n",
      "\n",
      "[41] Key:   ba <UNK> <UNK> <UNK> bi\n",
      "     Value: Total: 6,526 occurrences in 5,006 volumes (1704-2019, 211 bins)\n",
      "\n",
      "[42] Key:   ba <UNK> <UNK> <UNK> billion\n",
      "     Value: Total: 139 occurrences in 95 volumes (1968-2014, 18 bins)\n",
      "\n",
      "[43] Key:   ba <UNK> <UNK> <UNK> biological\n",
      "     Value: Total: 369 occurrences in 169 volumes (1982-2007, 24 bins)\n",
      "\n",
      "[44] Key:   ba <UNK> <UNK> <UNK> blood\n",
      "     Value: Total: 43 occurrences in 42 volumes (1982-2012, 17 bins)\n",
      "\n",
      "[45] Key:   ba <UNK> <UNK> <UNK> board\n",
      "     Value: Total: 41 occurrences in 41 volumes (1923-1992, 23 bins)\n",
      "\n",
      "[46] Key:   ba <UNK> <UNK> <UNK> buffalo\n",
      "     Value: Total: 107 occurrences in 54 volumes (1900-2006, 23 bins)\n",
      "\n",
      "[47] Key:   ba <UNK> <UNK> <UNK> bum\n",
      "     Value: Total: 284 occurrences in 252 volumes (1984-2019, 28 bins)\n",
      "\n",
      "[48] Key:   ba <UNK> <UNK> <UNK> bushmen\n",
      "     Value: Total: 62 occurrences in 62 volumes (1896-1928, 15 bins)\n",
      "\n",
      "[49] Key:   ba <UNK> <UNK> <UNK> business\n",
      "     Value: Total: 1,091 occurrences in 495 volumes (1966-2019, 49 bins)\n",
      "\n",
      "[50] Key:   ba <UNK> <UNK> <UNK> ca\n",
      "     Value: Total: 12,812 occurrences in 9,438 volumes (1796-2019, 197 bins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "db_peek_prefix(db, prefix=\"b\", n=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42684050",
   "metadata": {},
   "source": [
    "## **Optional: Count Database Items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00c555f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "/scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams.db\n",
      "Progress interval: every 10,000,000 items\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[     10,000,000] | elapsed     25.2s | rate 397,507 items/sec\n",
      "[     20,000,000] | elapsed     47.3s | rate 422,881 items/sec\n",
      "[     30,000,000] | elapsed     74.4s | rate 402,994 items/sec\n",
      "[     40,000,000] | elapsed     97.4s | rate 410,517 items/sec\n",
      "[     50,000,000] | elapsed    118.2s | rate 422,954 items/sec\n",
      "[     60,000,000] | elapsed    137.2s | rate 437,405 items/sec\n",
      "[     70,000,000] | elapsed    158.3s | rate 442,239 items/sec\n",
      "[     80,000,000] | elapsed    185.7s | rate 430,781 items/sec\n",
      "[     90,000,000] | elapsed    205.5s | rate 437,971 items/sec\n",
      "[    100,000,000] | elapsed    224.9s | rate 444,696 items/sec\n",
      "[    110,000,000] | elapsed    244.1s | rate 450,619 items/sec\n",
      "[    120,000,000] | elapsed    264.2s | rate 454,242 items/sec\n",
      "[    130,000,000] | elapsed    286.9s | rate 453,125 items/sec\n",
      "[    140,000,000] | elapsed    306.7s | rate 456,537 items/sec\n",
      "[    150,000,000] | elapsed    328.6s | rate 456,451 items/sec\n",
      "[    160,000,000] | elapsed    349.1s | rate 458,304 items/sec\n",
      "[    170,000,000] | elapsed    367.6s | rate 462,498 items/sec\n",
      "[    180,000,000] | elapsed    387.1s | rate 464,954 items/sec\n",
      "[    190,000,000] | elapsed    406.6s | rate 467,318 items/sec\n",
      "[    200,000,000] | elapsed    426.7s | rate 468,759 items/sec\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ COUNT COMPLETE                                                                                   │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 203,551,986                                                                               │\n",
      "│ Elapsed: 434.10s                                                                                 │\n",
      "│ Avg rate: 468,909 items/sec                                                                      │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "raw_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams.db'\n",
    "\n",
    "raw_count = count_db_items(raw_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b32fed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "...ch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_processed.db\n",
      "Progress interval: every 10,000,000 items\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[     10,000,000] | elapsed     60.1s | rate 166,494 items/sec\n",
      "[     20,000,000] | elapsed    117.4s | rate 170,428 items/sec\n",
      "[     30,000,000] | elapsed    174.7s | rate 171,764 items/sec\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ COUNT COMPLETE                                                                                   │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 30,027,257                                                                                │\n",
      "│ Elapsed: 174.72s                                                                                 │\n",
      "│ Avg rate: 171,857 items/sec                                                                      │\n",
      "│ Database: ...202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "filtered_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "filtered_count = count_db_items(filtered_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2edf4e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "...atch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db\n",
      "Progress interval: every 50,000,000 items\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[     50,000,000] | elapsed     22.7s | rate 2,199,962 items/sec\n",
      "[    100,000,000] | elapsed     45.6s | rate 2,194,238 items/sec\n",
      "[    150,000,000] | elapsed     68.6s | rate 2,185,296 items/sec\n",
      "[    200,000,000] | elapsed     92.2s | rate 2,169,326 items/sec\n",
      "[    250,000,000] | elapsed    115.2s | rate 2,170,430 items/sec\n",
      "[    300,000,000] | elapsed    138.9s | rate 2,159,459 items/sec\n",
      "[    350,000,000] | elapsed    161.4s | rate 2,169,175 items/sec\n",
      "[    400,000,000] | elapsed    184.3s | rate 2,170,344 items/sec\n",
      "[    450,000,000] | elapsed    207.3s | rate 2,170,534 items/sec\n",
      "[    500,000,000] | elapsed    230.3s | rate 2,171,405 items/sec\n",
      "[    550,000,000] | elapsed    253.3s | rate 2,171,658 items/sec\n",
      "[    600,000,000] | elapsed    276.0s | rate 2,173,674 items/sec\n",
      "[    650,000,000] | elapsed    298.9s | rate 2,174,973 items/sec\n",
      "[    700,000,000] | elapsed    321.3s | rate 2,178,454 items/sec\n",
      "[    750,000,000] | elapsed    344.2s | rate 2,178,839 items/sec\n",
      "[    800,000,000] | elapsed    367.1s | rate 2,179,283 items/sec\n",
      "[    850,000,000] | elapsed    389.7s | rate 2,180,985 items/sec\n",
      "[    900,000,000] | elapsed    412.7s | rate 2,180,827 items/sec\n",
      "[    950,000,000] | elapsed    438.9s | rate 2,164,609 items/sec\n",
      "[  1,000,000,000] | elapsed    461.0s | rate 2,169,317 items/sec\n",
      "[  1,050,000,000] | elapsed    483.4s | rate 2,172,130 items/sec\n",
      "[  1,100,000,000] | elapsed    506.1s | rate 2,173,362 items/sec\n",
      "[  1,150,000,000] | elapsed    529.2s | rate 2,173,045 items/sec\n",
      "[  1,200,000,000] | elapsed    552.2s | rate 2,173,005 items/sec\n",
      "[  1,250,000,000] | elapsed    575.1s | rate 2,173,497 items/sec\n",
      "[  1,300,000,000] | elapsed    598.1s | rate 2,173,600 items/sec\n",
      "[  1,350,000,000] | elapsed    620.8s | rate 2,174,553 items/sec\n",
      "[  1,400,000,000] | elapsed    643.8s | rate 2,174,616 items/sec\n",
      "[  1,450,000,000] | elapsed    667.6s | rate 2,171,828 items/sec\n",
      "[  1,500,000,000] | elapsed    690.6s | rate 2,172,092 items/sec\n",
      "[  1,550,000,000] | elapsed    714.6s | rate 2,168,947 items/sec\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ COUNT COMPLETE                                                                                   │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 1,586,548,214                                                                             │\n",
      "│ Elapsed: 731.25s                                                                                 │\n",
      "│ Avg rate: 2,169,650 items/sec                                                                    │\n",
      "│ Database: ...dk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "pivoted_count = count_db_items(pivoted_db, progress_interval=50_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec1afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "/scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Progress interval: every 50,000,000 items\n",
      "Grouping by: year_bin\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[     50,000,000] | elapsed     36.8s | rate 1,360,272 items/sec\n",
      "[    100,000,000] | elapsed     75.9s | rate 1,316,799 items/sec\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "pivoted_count_per_bin = count_db_items(pivoted_db, progress_interval=50_000_000, grouping='year_bin')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexichron)",
   "language": "python",
   "name": "lexichron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
