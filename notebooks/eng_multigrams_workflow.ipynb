{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a1e3e9b344be",
   "metadata": {},
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd936c0a392494",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f261bd4a6317873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.700012Z",
     "start_time": "2026-01-02T00:15:41.111800Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramprep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramprep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramprep.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramprep.utilities.peek import db_head, db_peek, db_peek_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5328f85c059eda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.870970Z",
     "start_time": "2026-01-02T00:15:41.713856Z"
    }
   },
   "outputs": [],
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = None\n",
    "release = '20200217'\n",
    "language = 'eng-us'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996606b00401532",
   "metadata": {},
   "source": [
    "## **Step 1: Download and Ingest**\n",
    "\n",
    "Specifying `combined_bigrams_download` will convert compound terms into single, hyphenated tokens. This process is case-sensitive, so we specify all common capitalization patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:50:58.847936Z",
     "start_time": "2025-12-24T20:09:09.627237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-05 09:25:22\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng-us/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "File range:           0 to 11144\n",
      "Total files:          11145\n",
      "Files to get:         11145\n",
      "Skipping:             0\n",
      "Download workers:     128\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         True\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████| 11145/11145 [1:38:40<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       11145\n",
      "Failed files:                0\n",
      "Total entries written:       1,391,714,327\n",
      "Write batches flushed:       3963\n",
      "Uncompressed data processed: 17.02 TB\n",
      "Processing throughput:       3003.07 MB/sec\n",
      "\n",
      "End Time: 2026-01-05 11:04:25.300122\n",
      "Total Runtime: 1:39:02.892968\n",
      "Time per file: 0:00:00.533234\n",
      "Files per hour: 6751.3\n"
     ]
    }
   ],
   "source": [
    "combined_bigrams_download = {\n",
    "    \"working class\", \"Working class\", \"Working Class\", \"working classes\", \"Working classes\", \"Working Classes\"\n",
    "    \"middle class\", \"Middle class\", \"Middle Class\", \"middle classes\", \"Middle classes\", \"Middle Classes\"\n",
    "    \"lower class\", \"Lower class\", \"Lower Class\", \"lower classes\", \"Lower classes\", \"Lower Classes\"\n",
    "    \"upper class\", \"Upper class\", \"Upper Class\", \"upper classes\", \"Upper classes\", \"Upper Classes\"\n",
    "    \"human being\", \"Human being\", \"Human Being\", \"human beings\", \"Human beings\", \"Human Beings\"\n",
    "}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=289,\n",
    "    overwrite_db=True,\n",
    "    workers=128,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=False,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:13:21.359494Z",
     "start_time": "2025-12-25T05:08:45.236033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-05 11:06:06\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "Target DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Temp directory:       .../edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Filter Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Min token length:     disabled\n",
      "Stopword filtering:   enabled (no stopwords loaded)\n",
      "Lemmatization:        enabled (no lemmatizer loaded)\n",
      "Always include:       52 token(s)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...Google_Books//20200217/eng-us/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    2.08M         74.9%          3/36        586·3·11      50.4k/s         41s      \n",
      "    15.43M        82.7%          6/36        576·6·18      216.1k/s       1m11s     \n",
      "    30.81M        83.8%         10/36       558·10·32      303.9k/s       1m41s     \n",
      "    64.42M        83.3%         13/36       548·13·39      490.4k/s       2m11s     \n",
      "   101.00M        83.3%         16/36       524·16·60      625.9k/s       2m41s     \n",
      "   140.97M        82.7%         19/36       502·19·79      736.5k/s       3m11s     \n",
      "   198.90M        83.0%         22/36       476·22·102     898.4k/s       3m41s     \n",
      "   263.92M        83.5%         25/36       438·25·137    1049.8k/s       4m11s     \n",
      "   327.17M        83.4%         28/36       403·28·169    1162.7k/s       4m41s     \n",
      "   402.37M        83.6%         32/36       377·32·191    1292.2k/s       5m11s     \n",
      "   489.86M        84.3%         34/36       347·34·219    1434.9k/s       5m41s     \n",
      "   574.36M        84.7%         36/36       304·36·260    1546.4k/s       6m11s     \n",
      "   671.65M        85.1%         36/36       270·36·294    1673.2k/s       6m41s     \n",
      "   773.43M        85.1%         36/36       238·36·326    1793.0k/s       7m11s     \n",
      "   864.26M        85.0%         36/36       183·36·381    1873.3k/s       7m41s     \n",
      "   956.81M        84.7%         36/36       137·36·427    1947.1k/s       8m11s     \n",
      "    1.05B         84.7%         36/36       104·36·460    2021.6k/s       8m41s     \n",
      "    1.15B         84.7%         36/36       66·36·498     2081.6k/s       9m11s     \n",
      "    1.25B         84.7%         36/36       36·36·528     2147.3k/s       9m41s     \n",
      "    1.33B         84.7%         19/36        0·19·581     2177.8k/s       10m11s    \n",
      "    1.38B         84.8%          8/36        0·8·592      2144.4k/s       10m41s    \n",
      "    1.39B         84.7%          1/36        0·1·599      2071.6k/s       11m11s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    1.39B         84.7%          0/36        0·0·600      2014.6k/s       11m30s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [41:20<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 310,002,815 items in 2480.2s (124,991 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 267,564,123 (estimated)                                                                   │\n",
      "│ Size: 279.04 GB                                                                                  │\n",
      "│ Database: ...h/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    \"brad\", \"brendan\", \"geoffrey\", \"greg\", \"brett\", \"jay\", \"matthew\", \"neil\", \"todd\",\n",
    "    \"allison\", \"anne\", \"carrie\", \"emily\", \"jill\", \"laurie\", \"kristen\", \"meredith\", \"sarah\",\n",
    "    \"darnell\", \"hakim\", \"jermaine\", \"kareem\", \"jamal\", \"leroy\", \"rasheed\", \"tremayne\", \"tyrone\",\n",
    "    \"aisha\", \"ebony\", \"keisha\", \"kenya\", \"latonya\", \"lakisha\", \"latoya\", \"tamika\", \"tanisha\",\n",
    "    \"joy\", \"love\", \"peace\", \"wonderful\", \"pleasure\", \"friend\", \"laughter\", \"happy\",\n",
    "    \"agony\", \"terrible\", \"horrible\", \"nasty\", \"evil\", \"war\", \"awful\", \"failure\"\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    compact_after_ingest=False,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "## **Step 3: Pivot to Yearly Indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:34:30.978547Z",
     "start_time": "2025-12-25T06:16:25.748406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-05 11:59:00\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Target DB:            ...k202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       ...ch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 30 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     3.29M            0.0x          570·30·0        162.1k/s          20s       \n",
      "     11.67M           0.0x          570·30·0        385.1k/s          30s       \n",
      "     17.72M           0.0x          570·30·0        439.3k/s          40s       \n",
      "     17.98M           0.0x          570·30·0        357.2k/s          50s       \n",
      "     17.98M           0.0x          570·30·0        298.0k/s         1m00s      \n",
      "     18.16M           3.1x          570·30·0        258.3k/s         1m10s      \n",
      "     25.91M          33.3x         560·30·10        322.6k/s         1m20s      \n",
      "     39.27M          42.9x         546·30·24        435.0k/s         1m30s      \n",
      "     49.65M          40.7x         541·30·29        495.1k/s         1m40s      \n",
      "     55.59M          37.1x         540·30·30        504.0k/s         1m50s      \n",
      "     57.70M          35.7x         540·30·30        479.5k/s         2m00s      \n",
      "     58.01M          35.5x         540·30·30        445.1k/s         2m10s      \n",
      "     60.40M          38.3x         537·30·33        430.5k/s         2m20s      \n",
      "     68.40M          43.7x         528·30·42        455.1k/s         2m30s      \n",
      "     78.22M          44.5x         520·30·50        488.0k/s         2m40s      \n",
      "     86.43M          43.3x         516·30·54        507.5k/s         2m50s      \n",
      "     92.31M          43.3x         513·30·57        511.9k/s         3m00s      \n",
      "     95.90M          43.0x         511·30·59        503.9k/s         3m10s      \n",
      "     98.11M          42.4x         510·30·60        489.9k/s         3m20s      \n",
      "    101.20M          44.0x         507·30·63        481.3k/s         3m30s      \n",
      "    108.03M          46.3x         500·30·70        490.3k/s         3m40s      \n",
      "    118.32M          47.0x         491·30·79        513.7k/s         3m50s      \n",
      "    124.73M          46.9x         486·30·84        519.0k/s         4m00s      \n",
      "    131.19M          47.6x         477·30·93        524.2k/s         4m10s      \n",
      "    137.44M          47.1x         470·30·100       528.0k/s         4m20s      \n",
      "    143.44M          48.3x         460·30·110       530.7k/s         4m30s      \n",
      "    153.60M          48.6x         446·30·124       548.0k/s         4m40s      \n",
      "    161.24M          48.6x         436·30·134       555.5k/s         4m50s      \n",
      "    170.56M          48.4x         421·30·149       568.0k/s         5m00s      \n",
      "    178.95M          48.0x         410·30·160       576.6k/s         5m10s      \n",
      "    187.46M          47.8x         395·30·175       585.2k/s         5m20s      \n",
      "    192.60M          48.0x         384·30·186       583.1k/s         5m30s      \n",
      "    202.23M          48.3x         358·30·212       594.2k/s         5m40s      \n",
      "    209.90M          48.2x         335·30·235       599.2k/s         5m50s      \n",
      "    217.89M          48.1x         312·30·258       604.7k/s         6m00s      \n",
      "    226.66M          47.7x         288·30·282       612.1k/s         6m10s      \n",
      "    236.62M          46.6x         272·30·298       622.2k/s         6m20s      \n",
      "    240.50M          46.7x         266·30·304       616.2k/s         6m30s      \n",
      "    251.55M          46.8x         246·30·324       628.4k/s         6m40s      \n",
      "    256.57M          46.3x         241·30·329       625.3k/s         6m50s      \n",
      "    264.84M          46.6x         225·30·345       630.1k/s         7m00s      \n",
      "    274.15M          46.2x         213·30·357       637.1k/s         7m10s      \n",
      "    281.55M          46.4x         198·30·372       639.4k/s         7m20s      \n",
      "    290.62M          46.2x         184·30·386       645.4k/s         7m30s      \n",
      "    297.00M          46.2x         174·30·396       645.2k/s         7m40s      \n",
      "    307.32M          45.9x         155·30·415       653.4k/s         7m50s      \n",
      "    313.07M          46.1x         141·30·429       651.8k/s         8m00s      \n",
      "    322.22M          45.8x         125·30·445       657.2k/s         8m10s      \n",
      "    327.89M          46.0x         113·30·457       655.4k/s         8m20s      \n",
      "    337.17M          45.8x         55·30·515        660.7k/s         8m30s      \n",
      "    340.62M          46.0x          0·14·586        654.7k/s         8m40s      \n",
      "    344.65M          46.4x          0·1·599         649.9k/s         8m50s      \n",
      "    344.84M          46.3x          0·1·599         638.2k/s         9m00s      \n",
      "    345.04M          46.3x          0·1·599         627.0k/s         9m10s      \n",
      "    345.26M          46.3x          0·1·599         616.2k/s         9m20s      \n",
      "    345.26M          46.3x          0·1·599         605.4k/s         9m30s      \n",
      "    345.26M          46.3x          0·1·599         595.0k/s         9m40s      \n",
      "    345.26M          46.3x          0·1·599         584.9k/s         9m50s      \n",
      "    345.26M          46.3x          0·1·599         575.1k/s         10m00s     \n",
      "    345.97M          46.4x          0·0·600         566.9k/s         10m10s     \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    345.97M          46.4x          0·0·600         558.7k/s         10m19s     \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:36<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 8,031,703,907 (estimated)                                                                 │\n",
      "│ Size: 117.23 GB                                                                                  │\n",
      "│ Database: ...tch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    compact_after_ingest=False,\n",
    "    progress_every_s=10.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:35:13.418972Z",
     "start_time": "2025-12-25T06:35:03.313440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1472] <UNK> <UNK> <UNK> <UNK> absence\n",
      "     Value: 5 occurrences in 3 documents\n",
      "\n",
      "[ 2] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordance\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordingly\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accrue\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1472] <UNK> <UNK> <UNK> <UNK> acre\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c3519e6ee50c4109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:06.913551Z",
     "start_time": "2025-12-25T06:39:56.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007e367656f6666726579203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] geoffrey <UNK> <UNK> <UNK> <UNK>\n",
      "     Value: 1,525 occurrences in 1,195 documents\n",
      "\n",
      "[ 2] Key:   [2019] geoffrey <UNK> <UNK> <UNK> architecture\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 3] Key:   [2019] geoffrey <UNK> <UNK> <UNK> army\n",
      "     Value: 7 occurrences in 7 documents\n",
      "\n",
      "[ 4] Key:   [2019] geoffrey <UNK> <UNK> <UNK> artisan\n",
      "     Value: 2 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [2019] geoffrey <UNK> <UNK> <UNK> awakening\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] geoffrey <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63e70a0ce04fb",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:20.607162Z",
     "start_time": "2025-12-25T06:40:10.425620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007763c554e4b3e2067656f6666726579:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1910] <UNK> geoffrey <UNK> <UNK> <UNK>\n",
      "     Value: 686 occurrences in 616 documents\n",
      "\n",
      "[ 2] Key:   [1910] <UNK> geoffrey <UNK> <UNK> baker\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1910] <UNK> geoffrey <UNK> <UNK> count\n",
      "     Value: 14 occurrences in 11 documents\n",
      "\n",
      "[ 4] Key:   [1910] <UNK> geoffrey <UNK> <UNK> course\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1910] <UNK> geoffrey <UNK> <UNK> father\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[1910] <UNK> geoffrey\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a850b70859d225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexichron)",
   "language": "python",
   "name": "lexichron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
