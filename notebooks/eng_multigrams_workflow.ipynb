{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a1e3e9b344be",
   "metadata": {},
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd936c0a392494",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f261bd4a6317873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.700012Z",
     "start_time": "2026-01-02T00:15:41.111800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramprep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramprep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramprep.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramprep.utilities.peek import db_head, db_peek, db_peek_prefix\n",
    "from ngramprep.utilities.count_items import count_db_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5328f85c059eda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.870970Z",
     "start_time": "2026-01-02T00:15:41.713856Z"
    }
   },
   "outputs": [],
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = None\n",
    "release = '20200217'\n",
    "language = 'eng'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996606b00401532",
   "metadata": {},
   "source": [
    "## **Step 1: Download and Ingest**\n",
    "\n",
    "Specifying `combined_bigrams_download` will convert compound terms into single, hyphenated tokens. This process is case-sensitive, so we specify all common capitalization patterns.\n",
    "\n",
    "If you're resuming downloads after an interruption, there may be a lag before you see any output. The RocksDB is being repaired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:50:58.847936Z",
     "start_time": "2025-12-24T20:09:09.627237Z"
    }
   },
   "outputs": [],
   "source": [
    "combined_bigrams_download = {\n",
    "    \"human being\", \"Human being\", \"Human Being\", \"human beings\", \"Human beings\", \"Human Beings\"\n",
    "}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=923,\n",
    "    overwrite_db=False,\n",
    "    workers=128,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams.\n",
    "\n",
    "`always_include_tokens` is applied after case-normalization, so we use all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:13:21.359494Z",
     "start_time": "2025-12-25T05:08:45.236033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-13 02:50:57\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       8\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Filter Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Whitelist mode:       ENABLED\n",
      "(All other filters are bypassed when whitelist is active)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ora/Google_Books/20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    3.35M         35.7%          4/36        593·4·3       84.8k/s         39s      \n",
      "    17.59M        47.0%          7/36        589·7·4       253.1k/s       1m09s     \n",
      "    43.00M        53.2%         10/36        583·10·7      432.0k/s       1m39s     \n",
      "    80.19M        52.9%         13/36        578·13·9      619.2k/s       2m09s     \n",
      "   117.83M        53.2%         16/36       558·16·26      738.8k/s       2m39s     \n",
      "   169.23M        52.2%         19/36       541·19·40      892.9k/s       3m09s     \n",
      "   234.75M        52.4%         22/36       525·22·53     1069.5k/s       3m39s     \n",
      "   301.49M        53.2%         24/36       498·25·77     1208.2k/s       4m09s     \n",
      "   375.50M        53.4%         28/36       476·28·96     1343.3k/s       4m39s     \n",
      "   476.51M        52.6%         31/36       462·31·107    1539.6k/s       5m09s     \n",
      "   565.67M        51.7%         34/36       432·34·134    1666.2k/s       5m39s     \n",
      "   670.16M        51.1%         36/36       401·35·164    1813.7k/s       6m09s     \n",
      "   783.07M        50.9%         36/36       376·36·188    1960.2k/s       6m39s     \n",
      "   894.44M        51.2%         36/36       348·36·216    2082.4k/s       7m09s     \n",
      "    1.01B         51.2%         36/36       324·36·240    2202.6k/s       7m39s     \n",
      "    1.12B         51.5%         36/36       299·36·265    2288.2k/s       8m09s     \n",
      "    1.23B         51.6%         36/36       277·36·287    2365.4k/s       8m39s     \n",
      "    1.36B         51.6%         36/36       259·36·305    2466.1k/s       9m09s     \n",
      "    1.46B         51.4%         36/36       226·36·338    2523.9k/s       9m39s     \n",
      "    1.57B         51.3%         36/36       196·36·368    2578.6k/s       10m09s    \n",
      "    1.69B         51.2%         36/36       170·36·394    2638.9k/s       10m39s    \n",
      "    1.80B         51.1%         36/36       135·36·429    2688.0k/s       11m09s    \n",
      "    1.91B         50.9%         36/36       105·36·459    2734.6k/s       11m39s    \n",
      "    2.03B         50.9%         36/36       75·36·489     2777.1k/s       12m09s    \n",
      "    2.13B         50.7%         36/36       35·36·529     2804.3k/s       12m39s    \n",
      "    2.24B         50.3%         33/36        0·33·567     2838.6k/s       13m09s    \n",
      "    2.33B         50.3%         21/36        0·21·579     2837.9k/s       13m39s    \n",
      "    2.39B         50.3%         15/36        0·15·585     2817.4k/s       14m09s    \n",
      "    2.44B         50.5%         10/36        0·10·590     2770.4k/s       14m39s    \n",
      "    2.47B         50.5%          5/36        0·5·595      2713.1k/s       15m09s    \n",
      "    2.47B         50.5%          2/36        0·2·598      2634.1k/s       15m39s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    2.48B         50.6%          0/36        0·0·600      2566.4k/s       16m06s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 8 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested:   6%|███▌                                                    | 38/600 [01:04<16:53]"
     ]
    }
   ],
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    'he', 'she',\n",
    "    'him', 'her',\n",
    "    'his', 'hers',\n",
    "    'himself', 'herself',\n",
    "    'man', 'woman',\n",
    "    'men', 'women',\n",
    "    'male', 'female',\n",
    "    'boy', 'girl',\n",
    "    'boys', 'girls',\n",
    "    'father', 'mother',\n",
    "    'fathers', 'mothers',\n",
    "    'son', 'daughter',\n",
    "    'sons', 'daughters',\n",
    "    'brother', 'sister',\n",
    "    'brothers', 'sisters'\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    compact_after_ingest=True,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "## **Step 3: Pivot to Yearly Indices**\n",
    "This function rearranges the filtered data such that each record has a year (or year bin) prefix. Ideal for time-series announcement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:34:30.978547Z",
     "start_time": "2025-12-25T06:16:25.748406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-12 21:50:30\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 600 density-based work units...\n",
      "Created 600 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 600 work units with 30 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     26.21M           0.0x          570·30·0        612.3k/s          43s       \n",
      "     31.91M           0.0x          570·30·0        438.3k/s         1m12s      \n",
      "     31.91M           0.0x          570·30·0        310.4k/s         1m42s      \n",
      "     34.73M          32.1x         560·30·10        261.5k/s         2m12s      \n",
      "     54.19M          33.9x         540·30·30        332.9k/s         2m42s      \n",
      "     65.82M          27.9x         540·30·30        341.4k/s         3m12s      \n",
      "     66.63M          30.1x         540·30·30        299.1k/s         3m42s      \n",
      "     73.36M          40.6x         524·30·46        290.2k/s         4m12s      \n",
      "     89.14M          40.5x         513·30·57        315.2k/s         4m42s      \n",
      "     98.30M          37.4x         510·30·60        314.2k/s         5m12s      \n",
      "     99.99M          40.3x         508·30·62        291.7k/s         5m42s      \n",
      "    110.28M          45.4x         493·30·77        295.8k/s         6m12s      \n",
      "    118.78M          47.2x         482·30·88        294.9k/s         6m42s      \n",
      "    127.14M          46.3x         466·30·104       293.7k/s         7m12s      \n",
      "    135.36M          49.3x         448·30·122       292.5k/s         7m42s      \n",
      "    145.72M          48.9x         430·30·140       295.7k/s         8m12s      \n",
      "    157.95M          48.2x         408·30·162       302.1k/s         8m42s      \n",
      "    167.61M          49.0x         385·30·185       303.2k/s         9m12s      \n",
      "    178.04M          49.2x         354·30·216       305.5k/s         9m42s      \n",
      "    188.03M          49.1x         326·30·244       306.8k/s         10m12s     \n",
      "    198.14M          49.4x         296·30·274       308.2k/s         10m42s     \n",
      "    211.19M          47.6x         276·30·294       313.9k/s         11m12s     \n",
      "    221.72M          47.6x         258·30·312       315.5k/s         11m42s     \n",
      "    230.85M          47.6x         244·30·326       315.0k/s         12m12s     \n",
      "    242.67M          47.5x         218·30·352       318.1k/s         12m42s     \n",
      "    253.62M          47.3x         199·30·371       319.9k/s         13m12s     \n",
      "    262.24M          47.5x         181·30·389       318.7k/s         13m42s     \n",
      "    273.44M          47.3x         158·30·412       320.6k/s         14m12s     \n",
      "    284.64M          47.4x         131·30·439       322.4k/s         14m42s     \n",
      "    294.27M          47.5x         107·30·463       322.4k/s         15m12s     \n",
      "    299.61M          48.0x          0·13·587        317.8k/s         15m42s     \n",
      "    301.25M          48.4x          0·1·599         309.7k/s         16m12s     \n",
      "    301.25M          48.4x          0·1·599         300.4k/s         16m42s     \n",
      "    301.25M          48.4x          0·1·599         291.7k/s         17m12s     \n",
      "    301.40M          48.6x          0·1·599         283.6k/s         17m42s     \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    301.40M          48.6x          0·0·600         280.4k/s         17m54s     \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:44<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         211.29 GB\n",
      "Compaction completed in 0:44:59\n",
      "Size before:             211.29 GB\n",
      "Size after:              517.97 GB\n",
      "Space saved:             -306.68 GB (-145.1%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 14,661,317,082 (estimated)                                                                │\n",
      "│ Size: 517.97 GB                                                                                  │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=False,\n",
    "    compact_after_ingest=True,\n",
    "    progress_every_s=30.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "## **Optional: Inspect Database Files**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "### `db_head`: Show first N records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:35:13.418972Z",
     "start_time": "2025-12-25T06:35:03.313440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   <UNK> <UNK> <UNK> abaca banana\n",
      "     Value: Total: 59 occurrences in 59 volumes (1992-2013, 17 bins)\n",
      "\n",
      "[ 2] Key:   <UNK> <UNK> <UNK> abaca industry\n",
      "     Value: Total: 267 occurrences in 223 volumes (1901-2019, 65 bins)\n",
      "\n",
      "[ 3] Key:   <UNK> <UNK> <UNK> abaca plant\n",
      "     Value: Total: 487 occurrences in 477 volumes (1887-2019, 111 bins)\n",
      "\n",
      "[ 4] Key:   <UNK> <UNK> <UNK> abaca plantation\n",
      "     Value: Total: 69 occurrences in 51 volumes (1898-1987, 19 bins)\n",
      "\n",
      "[ 5] Key:   <UNK> <UNK> <UNK> abaca production\n",
      "     Value: Total: 630 occurrences in 550 volumes (1949-2003, 39 bins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "db_head(db, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "### `db_peek`: Show Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3519e6ee50c4109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:06.913551Z",
     "start_time": "2025-12-25T06:39:56.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from c3a0203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "db_peek(db, start_key=\"à <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63e70a0ce04fb",
   "metadata": {},
   "source": [
    "### `db_peek_prefix`: Show records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:20.607162Z",
     "start_time": "2025-12-25T06:40:10.425620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 62:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   baa <UNK> <UNK> <UNK> also\n",
      "     Value: Total: 59 occurrences in 57 volumes (1921-1988, 22 bins)\n",
      "\n",
      "[ 2] Key:   baa <UNK> <UNK> <UNK> authority\n",
      "     Value: Total: 299 occurrences in 260 volumes (1967-2019, 38 bins)\n",
      "\n",
      "[ 3] Key:   baa <UNK> <UNK> <UNK> average\n",
      "     Value: Total: 284 occurrences in 45 volumes (1953-2003, 23 bins)\n",
      "\n",
      "[ 4] Key:   baa <UNK> <UNK> <UNK> baa\n",
      "     Value: Total: 127 occurrences in 121 volumes (1926-2017, 40 bins)\n",
      "\n",
      "[ 5] Key:   baa <UNK> <UNK> <UNK> better\n",
      "     Value: Total: 147 occurrences in 121 volumes (1966-2016, 35 bins)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "db_peek_prefix(db, prefix=\"b\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42684050",
   "metadata": {},
   "source": [
    "## **Optional: Count Database Items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c555f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "/scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams.db\n",
      "Progress interval: every 10,000,000 items\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[     10,000,000] | elapsed     25.2s | rate 397,507 items/sec\n",
      "[     20,000,000] | elapsed     47.3s | rate 422,881 items/sec\n",
      "[     30,000,000] | elapsed     74.4s | rate 402,994 items/sec\n",
      "[     40,000,000] | elapsed     97.4s | rate 410,517 items/sec\n",
      "[     50,000,000] | elapsed    118.2s | rate 422,954 items/sec\n",
      "[     60,000,000] | elapsed    137.2s | rate 437,405 items/sec\n",
      "[     70,000,000] | elapsed    158.3s | rate 442,239 items/sec\n",
      "[     80,000,000] | elapsed    185.7s | rate 430,781 items/sec\n",
      "[     90,000,000] | elapsed    205.5s | rate 437,971 items/sec\n",
      "[    100,000,000] | elapsed    224.9s | rate 444,696 items/sec\n",
      "[    110,000,000] | elapsed    244.1s | rate 450,619 items/sec\n",
      "[    120,000,000] | elapsed    264.2s | rate 454,242 items/sec\n",
      "[    130,000,000] | elapsed    286.9s | rate 453,125 items/sec\n",
      "[    140,000,000] | elapsed    306.7s | rate 456,537 items/sec\n",
      "[    150,000,000] | elapsed    328.6s | rate 456,451 items/sec\n",
      "[    160,000,000] | elapsed    349.1s | rate 458,304 items/sec\n",
      "[    170,000,000] | elapsed    367.6s | rate 462,498 items/sec\n",
      "[    180,000,000] | elapsed    387.1s | rate 464,954 items/sec\n",
      "[    190,000,000] | elapsed    406.6s | rate 467,318 items/sec\n",
      "[    200,000,000] | elapsed    426.7s | rate 468,759 items/sec\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ COUNT COMPLETE                                                                                   │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 203,551,986                                                                               │\n",
      "│ Elapsed: 434.10s                                                                                 │\n",
      "│ Avg rate: 468,909 items/sec                                                                      │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "raw_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams.db'\n",
    "\n",
    "raw_count = count_db_items(raw_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32fed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "/scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Progress interval: every 10,000,000 items\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "filtered_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "filtered_count = count_db_items(filtered_db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf4e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "...atch/edk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db\n",
      "Progress interval: every 50,000,000 items\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[     50,000,000] | elapsed     22.7s | rate 2,199,962 items/sec\n",
      "[    100,000,000] | elapsed     45.6s | rate 2,194,238 items/sec\n",
      "[    150,000,000] | elapsed     68.6s | rate 2,185,296 items/sec\n",
      "[    200,000,000] | elapsed     92.2s | rate 2,169,326 items/sec\n",
      "[    250,000,000] | elapsed    115.2s | rate 2,170,430 items/sec\n",
      "[    300,000,000] | elapsed    138.9s | rate 2,159,459 items/sec\n",
      "[    350,000,000] | elapsed    161.4s | rate 2,169,175 items/sec\n",
      "[    400,000,000] | elapsed    184.3s | rate 2,170,344 items/sec\n",
      "[    450,000,000] | elapsed    207.3s | rate 2,170,534 items/sec\n",
      "[    500,000,000] | elapsed    230.3s | rate 2,171,405 items/sec\n",
      "[    550,000,000] | elapsed    253.3s | rate 2,171,658 items/sec\n",
      "[    600,000,000] | elapsed    276.0s | rate 2,173,674 items/sec\n",
      "[    650,000,000] | elapsed    298.9s | rate 2,174,973 items/sec\n",
      "[    700,000,000] | elapsed    321.3s | rate 2,178,454 items/sec\n",
      "[    750,000,000] | elapsed    344.2s | rate 2,178,839 items/sec\n",
      "[    800,000,000] | elapsed    367.1s | rate 2,179,283 items/sec\n",
      "[    850,000,000] | elapsed    389.7s | rate 2,180,985 items/sec\n",
      "[    900,000,000] | elapsed    412.7s | rate 2,180,827 items/sec\n",
      "[    950,000,000] | elapsed    438.9s | rate 2,164,609 items/sec\n",
      "[  1,000,000,000] | elapsed    461.0s | rate 2,169,317 items/sec\n",
      "[  1,050,000,000] | elapsed    483.4s | rate 2,172,130 items/sec\n",
      "[  1,100,000,000] | elapsed    506.1s | rate 2,173,362 items/sec\n",
      "[  1,150,000,000] | elapsed    529.2s | rate 2,173,045 items/sec\n",
      "[  1,200,000,000] | elapsed    552.2s | rate 2,173,005 items/sec\n",
      "[  1,250,000,000] | elapsed    575.1s | rate 2,173,497 items/sec\n",
      "[  1,300,000,000] | elapsed    598.1s | rate 2,173,600 items/sec\n",
      "[  1,350,000,000] | elapsed    620.8s | rate 2,174,553 items/sec\n",
      "[  1,400,000,000] | elapsed    643.8s | rate 2,174,616 items/sec\n",
      "[  1,450,000,000] | elapsed    667.6s | rate 2,171,828 items/sec\n",
      "[  1,500,000,000] | elapsed    690.6s | rate 2,172,092 items/sec\n",
      "[  1,550,000,000] | elapsed    714.6s | rate 2,168,947 items/sec\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ COUNT COMPLETE                                                                                   │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 1,586,548,214                                                                             │\n",
      "│ Elapsed: 731.25s                                                                                 │\n",
      "│ Avg rate: 2,169,650 items/sec                                                                    │\n",
      "│ Database: ...dk202/NLP_corpora/Google_Books/20200217/eng-fiction/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "pivoted_count = count_db_items(pivoted_db, progress_interval=50_000_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec1afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATABASE ITEM COUNTER\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "/scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Progress interval: every 50,000,000 items\n",
      "Grouping by: year_bin\n",
      "\n",
      "COUNTING\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[     50,000,000] | elapsed     36.8s | rate 1,360,272 items/sec\n",
      "[    100,000,000] | elapsed     75.9s | rate 1,316,799 items/sec\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "pivoted_count_per_bin = count_db_items(pivoted_db, progress_interval=50_000_000, grouping='year_bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0930e191",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexichron)",
   "language": "python",
   "name": "lexichron"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
