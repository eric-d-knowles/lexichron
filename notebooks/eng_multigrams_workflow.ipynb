{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a1e3e9b344be",
   "metadata": {},
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd936c0a392494",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f261bd4a6317873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.700012Z",
     "start_time": "2026-01-02T00:15:41.111800Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramprep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramprep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramprep.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramprep.utilities.peek import db_head, db_peek, db_peek_prefix\n",
    "from ngramprep.utilities.count_items import count_db_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5328f85c059eda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.870970Z",
     "start_time": "2026-01-02T00:15:41.713856Z"
    }
   },
   "outputs": [],
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = None\n",
    "release = '20200217'\n",
    "language = 'eng-us'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996606b00401532",
   "metadata": {},
   "source": [
    "## **Step 1: Download and Ingest**\n",
    "\n",
    "Specifying `combined_bigrams_download` will convert compound terms into single, hyphenated tokens. This process is case-sensitive, so we specify all common capitalization patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:50:58.847936Z",
     "start_time": "2025-12-24T20:09:09.627237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-05 09:25:22\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng-us/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "File range:           0 to 11144\n",
      "Total files:          11145\n",
      "Files to get:         11145\n",
      "Skipping:             0\n",
      "Download workers:     128\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         True\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████| 11145/11145 [1:38:40<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       11145\n",
      "Failed files:                0\n",
      "Total entries written:       1,391,714,327\n",
      "Write batches flushed:       3963\n",
      "Uncompressed data processed: 17.02 TB\n",
      "Processing throughput:       3003.07 MB/sec\n",
      "\n",
      "End Time: 2026-01-05 11:04:25.300122\n",
      "Total Runtime: 1:39:02.892968\n",
      "Time per file: 0:00:00.533234\n",
      "Files per hour: 6751.3\n"
     ]
    }
   ],
   "source": [
    "combined_bigrams_download = {\n",
    "    \"working class\", \"Working class\", \"Working Class\", \"working classes\", \"Working classes\", \"Working Classes\"\n",
    "    \"middle class\", \"Middle class\", \"Middle Class\", \"middle classes\", \"Middle classes\", \"Middle Classes\"\n",
    "    \"lower class\", \"Lower class\", \"Lower Class\", \"lower classes\", \"Lower classes\", \"Lower Classes\"\n",
    "    \"upper class\", \"Upper class\", \"Upper Class\", \"upper classes\", \"Upper classes\", \"Upper Classes\"\n",
    "    \"human being\", \"Human being\", \"Human Being\", \"human beings\", \"Human beings\", \"Human Beings\"\n",
    "}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=289,\n",
    "    overwrite_db=True,\n",
    "    workers=128,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=False,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams.\n",
    "\n",
    "`always_include_tokens` is applied after case-normalization, so we use all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:13:21.359494Z",
     "start_time": "2025-12-25T05:08:45.236033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-07 01:44:29\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "Target DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Temp directory:       .../edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       8\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Filter Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Min token length:     3\n",
      "Stopword filtering:   enabled (no stopwords loaded)\n",
      "Lemmatization:        enabled (no lemmatizer loaded)\n",
      "Always include:       30 token(s)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...Google_Books//20200217/eng-us/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading whitelist...\n",
      "Loaded 20,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 600 density-based work units...\n",
      "Created 600 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    4.09M         77.2%          4/36        592·4·4       99.3k/s         41s      \n",
      "    16.41M        85.6%          7/36        585·7·8       230.3k/s       1m11s     \n",
      "    40.95M        86.9%         10/36       578·10·12      404.6k/s       1m41s     \n",
      "    68.93M        86.6%         13/36       554·13·33      525.4k/s       2m11s     \n",
      "   112.00M        85.9%         17/36       534·17·49      694.6k/s       2m41s     \n",
      "   158.29M        85.3%         20/36       511·20·69      827.7k/s       3m11s     \n",
      "   217.07M        85.1%         23/36       485·23·92      981.2k/s       3m41s     \n",
      "   281.68M        85.3%         26/36       460·26·114    1121.3k/s       4m11s     \n",
      "   360.37M        85.6%         29/36       428·29·143    1281.6k/s       4m41s     \n"
     ]
    }
   ],
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    'he', 'she',\n",
    "    'him', 'her',\n",
    "    'his', 'hers',\n",
    "    'himself', 'herself',\n",
    "    'man', 'woman',\n",
    "    'men', 'women',\n",
    "    'male', 'female',\n",
    "    'boy', 'girl',\n",
    "    'boys', 'girls',\n",
    "    'father', 'mother',\n",
    "    'fathers', 'mothers',\n",
    "    'son', 'daughter',\n",
    "    'sons', 'daughters',\n",
    "    'brother', 'sister',\n",
    "    'brothers', 'sisters'\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    compact_after_ingest=True,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "## **Step 3: Pivot to Yearly Indices**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3889bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:34:30.978547Z",
     "start_time": "2025-12-25T06:16:25.748406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-07 00:46:39\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Target DB:            ...k202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       ...ch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Sampling database to create 600 density-based work units...\n",
      "Created 600 balanced work units based on data density\n",
      "Cached partition results for future use\n",
      "\n",
      "Phase 2: Processing 600 work units with 30 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     7.75M            0.0x          570·30·0        326.7k/s          24s       \n",
      "     16.25M           0.0x          570·30·0        482.0k/s          34s       \n",
      "     16.78M           0.0x          570·30·0        383.8k/s          44s       \n",
      "     16.78M           0.0x          570·30·0        312.4k/s          54s       \n",
      "     17.00M           5.4x          570·30·0        266.9k/s         1m03s      \n",
      "     31.34M          42.8x         552·30·18        425.2k/s         1m13s      \n",
      "     44.12M          42.2x         544·30·26        526.9k/s         1m23s      \n",
      "     53.55M          38.3x         540·30·30        571.5k/s         1m33s      \n",
      "     55.90M          36.7x         540·30·30        539.0k/s         1m43s      \n",
      "     56.09M          36.6x         540·30·30        493.3k/s         1m53s      \n",
      "     58.50M          40.5x         537·30·33        472.9k/s         2m03s      \n",
      "     70.85M          46.8x         523·30·47        530.0k/s         2m13s      \n",
      "     84.66M          45.9x         513·30·57        589.0k/s         2m23s      \n",
      "     91.80M          43.9x         512·30·58        597.2k/s         2m33s      \n",
      "     94.93M          43.3x         510·30·60        579.8k/s         2m43s      \n",
      "     95.98M          42.8x         510·30·60        552.5k/s         2m53s      \n",
      "     97.83M          45.3x         508·30·62        532.5k/s         3m03s      \n",
      "    110.38M          49.5x         494·30·76        569.8k/s         3m13s      \n",
      "    122.85M          48.2x         484·30·86        603.0k/s         3m23s      \n",
      "    126.33M          48.5x         481·30·89        591.1k/s         3m33s      \n",
      "    135.58M          48.4x         465·30·105       606.0k/s         3m43s      \n",
      "    139.67M          48.1x         461·30·109       597.6k/s         3m53s      \n",
      "    148.44M          49.3x         446·30·124       609.1k/s         4m03s      \n",
      "    157.75M          49.0x         436·30·134       621.7k/s         4m13s      \n",
      "    163.78M          49.0x         427·30·143       621.0k/s         4m23s      \n",
      "    173.74M          48.8x         408·30·162       634.7k/s         4m33s      \n",
      "    180.34M          48.4x         399·30·171       635.7k/s         4m43s      \n",
      "    187.74M          48.8x         384·30·186       639.1k/s         4m53s      \n",
      "    194.16M          49.1x         368·30·202       639.3k/s         5m03s      \n",
      "    203.37M          48.6x         348·30·222       648.3k/s         5m13s      \n",
      "    210.29M          48.6x         329·30·241       649.6k/s         5m23s      \n",
      "    216.36M          48.5x         314·30·256       648.3k/s         5m33s      \n",
      "    224.76M          48.7x         288·30·282       653.9k/s         5m43s      \n",
      "    233.59M          47.7x         276·30·294       660.4k/s         5m53s      \n",
      "    238.45M          47.6x         270·30·300       655.5k/s         6m03s      \n",
      "    248.06M          47.5x         252·30·318       663.7k/s         6m13s      \n",
      "    254.91M          47.2x         241·30·329       664.3k/s         6m23s      \n",
      "    260.59M          47.6x         230·30·340       661.9k/s         6m33s      \n",
      "    271.95M          47.2x         213·30·357       673.6k/s         6m43s      \n",
      "    276.39M          47.1x         208·30·362       668.0k/s         6m53s      \n",
      "    286.55M          47.1x         189·30·381       676.2k/s         7m03s      \n",
      "    293.06M          46.7x         180·30·390       675.7k/s         7m13s      \n",
      "    301.81M          47.0x         160·30·410       680.2k/s         7m23s      \n",
      "    308.48M          46.7x         150·30·420       679.9k/s         7m33s      \n",
      "    316.88M          46.8x         130·30·440       683.3k/s         7m43s      \n",
      "    323.79M          46.5x         118·30·452       683.5k/s         7m53s      \n",
      "    330.97M          46.8x         98·30·472        684.2k/s         8m03s      \n",
      "    337.62M          46.7x          5·30·565        683.8k/s         8m13s      \n",
      "    341.70M          47.0x          0·6·594         678.3k/s         8m23s      \n",
      "    343.52M          47.1x          0·1·599         668.7k/s         8m33s      \n",
      "    343.71M          47.1x          0·1·599         656.3k/s         8m43s      \n",
      "    343.71M          47.1x          0·1·599         644.0k/s         8m53s      \n",
      "    343.71M          47.1x          0·1·599         632.1k/s         9m03s      \n",
      "    343.71M          47.1x          0·1·599         620.7k/s         9m13s      \n",
      "    343.71M          47.1x          0·1·599         609.7k/s         9m23s      \n",
      "    344.50M          47.2x          0·0·600         600.5k/s         9m33s      \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    344.50M          47.2x          0·0·600         591.0k/s         9m42s      \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:35<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         117.94 GB\n",
      "Compaction completed in 0:25:35\n",
      "Size before:             117.94 GB\n",
      "Size after:              288.56 GB\n",
      "Space saved:             -170.62 GB (-144.7%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 8,129,170,695 (estimated)                                                                 │\n",
      "│ Size: 288.56 GB                                                                                  │\n",
      "│ Database: ...tch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=False,\n",
    "    compact_after_ingest=True,\n",
    "    progress_every_s=10.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:35:13.418972Z",
     "start_time": "2025-12-25T06:35:03.313440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1472] <UNK> <UNK> <UNK> <UNK> absence\n",
      "     Value: 5 occurrences in 3 documents\n",
      "\n",
      "[ 2] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordance\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordingly\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accrue\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1472] <UNK> <UNK> <UNK> <UNK> acre\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3519e6ee50c4109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:06.913551Z",
     "start_time": "2025-12-25T06:39:56.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007e368657273656c66203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] herself <UNK> <UNK> <UNK> <UNK>\n",
      "     Value: 15,021 occurrences in 14,857 documents\n",
      "\n",
      "[ 2] Key:   [2019] herself <UNK> <UNK> <UNK> able\n",
      "     Value: 53 occurrences in 51 documents\n",
      "\n",
      "[ 3] Key:   [2019] herself <UNK> <UNK> <UNK> absence\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [2019] herself <UNK> <UNK> <UNK> absurd\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [2019] herself <UNK> <UNK> <UNK> abyss\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] herself <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63e70a0ce04fb",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:20.607162Z",
     "start_time": "2025-12-25T06:40:10.425620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007763c554e4b3e2068696d73656c66:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1910] <UNK> himself <UNK> <UNK> <UNK>\n",
      "     Value: 94,539 occurrences in 90,099 documents\n",
      "\n",
      "[ 2] Key:   [1910] <UNK> himself <UNK> <UNK> ability\n",
      "     Value: 35 occurrences in 29 documents\n",
      "\n",
      "[ 3] Key:   [1910] <UNK> himself <UNK> <UNK> abject\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 4] Key:   [1910] <UNK> himself <UNK> <UNK> able\n",
      "     Value: 99 occurrences in 99 documents\n",
      "\n",
      "[ 5] Key:   [1910] <UNK> himself <UNK> <UNK> aboriginal\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[1910] <UNK> himself\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b32fed5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting items in: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Database Item Counter\n",
      "=================================================================\n",
      "Progress: 10,000,000 items | 33.0s elapsed | 302,608 items/sec\n",
      "Progress: 20,000,000 items | 60.6s elapsed | 329,989 items/sec\n",
      "Progress: 30,000,000 items | 85.6s elapsed | 350,311 items/sec\n",
      "Progress: 40,000,000 items | 112.1s elapsed | 356,839 items/sec\n",
      "Progress: 50,000,000 items | 140.8s elapsed | 355,066 items/sec\n",
      "Progress: 60,000,000 items | 166.3s elapsed | 360,773 items/sec\n",
      "Progress: 70,000,000 items | 191.4s elapsed | 365,723 items/sec\n",
      "Progress: 80,000,000 items | 220.7s elapsed | 362,486 items/sec\n",
      "Progress: 90,000,000 items | 245.6s elapsed | 366,506 items/sec\n",
      "Progress: 100,000,000 items | 271.6s elapsed | 368,190 items/sec\n",
      "Progress: 110,000,000 items | 298.8s elapsed | 368,125 items/sec\n",
      "Progress: 120,000,000 items | 326.4s elapsed | 367,659 items/sec\n",
      "Progress: 130,000,000 items | 350.7s elapsed | 370,698 items/sec\n",
      "Progress: 140,000,000 items | 378.9s elapsed | 369,444 items/sec\n",
      "Progress: 150,000,000 items | 405.3s elapsed | 370,071 items/sec\n",
      "Progress: 160,000,000 items | 437.9s elapsed | 365,387 items/sec\n",
      "Progress: 170,000,000 items | 468.5s elapsed | 362,864 items/sec\n",
      "=================================================================\n",
      "FINAL COUNT: 172,248,874 items\n",
      "Total Time:  475.33 seconds\n",
      "=================================================================\n",
      "Filtered database contains: 172,248,874 n-grams\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Count items in the filtered input database\n",
    "filtered_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_processed.db'\n",
    "print(f\"Counting items in: {filtered_db}\")\n",
    "filtered_count = count_db_items(filtered_db)\n",
    "print(f\"Filtered database contains: {filtered_count:,} n-grams\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edf4e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting items in: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db\n",
      "Database Item Counter\n",
      "=================================================================\n",
      "Progress: 10,000,000 items | 4.5s elapsed | 2,245,995 items/sec\n",
      "Progress: 20,000,000 items | 10.8s elapsed | 1,845,493 items/sec\n",
      "Progress: 30,000,000 items | 16.2s elapsed | 1,855,844 items/sec\n",
      "Progress: 40,000,000 items | 20.7s elapsed | 1,931,639 items/sec\n",
      "Progress: 50,000,000 items | 25.2s elapsed | 1,983,426 items/sec\n",
      "Progress: 60,000,000 items | 29.7s elapsed | 2,020,510 items/sec\n",
      "Progress: 70,000,000 items | 34.2s elapsed | 2,047,897 items/sec\n",
      "Progress: 80,000,000 items | 38.6s elapsed | 2,070,888 items/sec\n",
      "Progress: 90,000,000 items | 43.2s elapsed | 2,082,422 items/sec\n",
      "Progress: 100,000,000 items | 48.3s elapsed | 2,068,789 items/sec\n",
      "Progress: 110,000,000 items | 52.9s elapsed | 2,077,580 items/sec\n",
      "Progress: 120,000,000 items | 57.4s elapsed | 2,091,591 items/sec\n",
      "Progress: 130,000,000 items | 61.8s elapsed | 2,102,148 items/sec\n",
      "Progress: 140,000,000 items | 66.3s elapsed | 2,110,427 items/sec\n",
      "Progress: 150,000,000 items | 70.9s elapsed | 2,114,667 items/sec\n",
      "Progress: 160,000,000 items | 75.4s elapsed | 2,121,698 items/sec\n",
      "Progress: 170,000,000 items | 80.0s elapsed | 2,124,664 items/sec\n",
      "Progress: 180,000,000 items | 84.6s elapsed | 2,126,873 items/sec\n",
      "Progress: 190,000,000 items | 89.2s elapsed | 2,129,805 items/sec\n",
      "Progress: 200,000,000 items | 93.8s elapsed | 2,131,153 items/sec\n",
      "Progress: 210,000,000 items | 98.5s elapsed | 2,132,385 items/sec\n",
      "Progress: 220,000,000 items | 103.0s elapsed | 2,134,898 items/sec\n",
      "Progress: 230,000,000 items | 107.7s elapsed | 2,136,093 items/sec\n",
      "Progress: 240,000,000 items | 112.3s elapsed | 2,137,489 items/sec\n",
      "Progress: 250,000,000 items | 116.9s elapsed | 2,139,066 items/sec\n",
      "Progress: 260,000,000 items | 121.5s elapsed | 2,139,865 items/sec\n",
      "Progress: 270,000,000 items | 126.1s elapsed | 2,141,099 items/sec\n",
      "Progress: 280,000,000 items | 130.7s elapsed | 2,142,609 items/sec\n",
      "Progress: 290,000,000 items | 135.3s elapsed | 2,143,753 items/sec\n",
      "Progress: 300,000,000 items | 140.0s elapsed | 2,143,471 items/sec\n",
      "Progress: 310,000,000 items | 145.7s elapsed | 2,127,297 items/sec\n",
      "Progress: 320,000,000 items | 153.5s elapsed | 2,084,985 items/sec\n",
      "Progress: 330,000,000 items | 158.2s elapsed | 2,085,847 items/sec\n",
      "Progress: 340,000,000 items | 164.6s elapsed | 2,066,006 items/sec\n",
      "Progress: 350,000,000 items | 169.2s elapsed | 2,069,077 items/sec\n",
      "Progress: 360,000,000 items | 174.1s elapsed | 2,067,804 items/sec\n",
      "Progress: 370,000,000 items | 178.5s elapsed | 2,072,595 items/sec\n",
      "Progress: 380,000,000 items | 184.5s elapsed | 2,059,992 items/sec\n",
      "Progress: 390,000,000 items | 189.2s elapsed | 2,061,008 items/sec\n",
      "Progress: 400,000,000 items | 193.8s elapsed | 2,063,508 items/sec\n",
      "Progress: 410,000,000 items | 198.4s elapsed | 2,066,226 items/sec\n",
      "Progress: 420,000,000 items | 204.0s elapsed | 2,058,493 items/sec\n",
      "Progress: 430,000,000 items | 208.7s elapsed | 2,060,078 items/sec\n",
      "Progress: 440,000,000 items | 213.4s elapsed | 2,062,226 items/sec\n"
     ]
    }
   ],
   "source": [
    "# Count items in the pivoted output database\n",
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "print(f\"Counting items in: {pivoted_db}\")\n",
    "pivoted_count = count_db_items(pivoted_db)\n",
    "print(f\"Pivoted database contains: {pivoted_count:,} n-grams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ec1afb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexichron)",
   "language": "python",
   "name": "lexichron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
