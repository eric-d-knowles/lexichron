{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a1e3e9b344be",
   "metadata": {},
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd936c0a392494",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f261bd4a6317873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.700012Z",
     "start_time": "2026-01-02T00:15:41.111800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramprep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramprep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramprep.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramprep.utilities.peek import db_head, db_peek, db_peek_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5328f85c059eda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.870970Z",
     "start_time": "2026-01-02T00:15:41.713856Z"
    }
   },
   "outputs": [],
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = None\n",
    "release = '20200217'\n",
    "language = 'eng-us'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996606b00401532",
   "metadata": {},
   "source": [
    "## **Step 1: Download and Ingest**\n",
    "\n",
    "Specifying `combined_bigrams_download` will convert compound terms into single, hyphenated tokens. This process is case-sensitive, so we specify all common capitalization patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:50:58.847936Z",
     "start_time": "2025-12-24T20:09:09.627237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-05 09:25:22\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng-us/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "File range:           0 to 11144\n",
      "Total files:          11145\n",
      "Files to get:         11145\n",
      "Skipping:             0\n",
      "Download workers:     128\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         True\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████| 11145/11145 [1:38:40<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       11145\n",
      "Failed files:                0\n",
      "Total entries written:       1,391,714,327\n",
      "Write batches flushed:       3963\n",
      "Uncompressed data processed: 17.02 TB\n",
      "Processing throughput:       3003.07 MB/sec\n",
      "\n",
      "End Time: 2026-01-05 11:04:25.300122\n",
      "Total Runtime: 1:39:02.892968\n",
      "Time per file: 0:00:00.533234\n",
      "Files per hour: 6751.3\n"
     ]
    }
   ],
   "source": [
    "combined_bigrams_download = {\n",
    "    \"working class\", \"Working class\", \"Working Class\", \"working classes\", \"Working classes\", \"Working Classes\"\n",
    "    \"middle class\", \"Middle class\", \"Middle Class\", \"middle classes\", \"Middle classes\", \"Middle Classes\"\n",
    "    \"lower class\", \"Lower class\", \"Lower Class\", \"lower classes\", \"Lower classes\", \"Lower Classes\"\n",
    "    \"upper class\", \"Upper class\", \"Upper Class\", \"upper classes\", \"Upper classes\", \"Upper Classes\"\n",
    "    \"human being\", \"Human being\", \"Human Being\", \"human beings\", \"Human beings\", \"Human Beings\"\n",
    "}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=289,\n",
    "    overwrite_db=True,\n",
    "    workers=128,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=False,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams.\n",
    "\n",
    "`always_include_tokens` is applied after case-normalization, so we use all lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:13:21.359494Z",
     "start_time": "2025-12-25T05:08:45.236033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-06 19:41:08\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "Target DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Temp directory:       .../edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Filter Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Min token length:     disabled\n",
      "Stopword filtering:   enabled (no stopwords loaded)\n",
      "Lemmatization:        enabled (no lemmatizer loaded)\n",
      "Always include:       30 token(s)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...Google_Books//20200217/eng-us/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    2.85M         74.7%          4/36        591·4·5       70.5k/s         40s      \n",
      "    14.54M        84.3%          7/36        572·7·21      206.6k/s       1m10s     \n",
      "    38.45M        86.2%         10/36       554·10·36      383.0k/s       1m40s     \n",
      "    73.11M        86.0%         14/36       540·14·46      560.8k/s       2m10s     \n",
      "   109.67M        85.7%         17/36       513·17·70      683.9k/s       2m40s     \n",
      "   159.69M        86.0%         20/36       496·20·84      838.8k/s       3m10s     \n",
      "   215.58M        86.3%         23/36       466·23·111     978.3k/s       3m40s     \n",
      "   277.98M        86.1%         26/36       428·26·146    1110.2k/s       4m10s     \n",
      "   343.55M        85.4%         30/36       384·30·186    1225.5k/s       4m40s     \n",
      "   427.52M        84.7%         33/36       353·33·214    1377.6k/s       5m10s     \n",
      "   507.51M        84.9%         35/36       317·35·248    1491.1k/s       5m40s     \n",
      "   595.85M        85.2%         36/36       274·36·290    1608.8k/s       6m10s     \n",
      "   697.12M        85.7%         36/36       250·36·314    1741.2k/s       6m40s     \n",
      "   789.15M        86.1%         36/36       219·36·345    1833.7k/s       7m10s     \n",
      "   882.89M        86.2%         36/36       176·36·388    1917.8k/s       7m40s     \n",
      "   984.69M        85.9%         36/36       130·36·434    2008.1k/s       8m10s     \n",
      "    1.07B         85.9%         35/36       74·35·491     2064.4k/s       8m40s     \n",
      "    1.17B         85.8%         36/36       28·36·536     2131.2k/s       9m10s     \n",
      "    1.28B         85.9%         32/36        0·32·568     2202.1k/s       9m40s     \n",
      "    1.34B         85.9%         15/36        0·15·585     2196.9k/s       10m10s    \n",
      "    1.38B         86.0%         10/36        0·10·590     2157.1k/s       10m40s    \n",
      "    1.39B         86.0%          0/36        0·0·600      2076.0k/s       11m10s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    1.39B         86.0%          0/36        0·0·600      2049.2k/s       11m19s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [44:36<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 325,177,784 items in 2676.3s (121,505 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 282,846,041 (estimated)                                                                   │\n",
      "│ Size: 297.18 GB                                                                                  │\n",
      "│ Database: ...h/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    'he', 'she',\n",
    "    'him', 'her',\n",
    "    'his', 'hers',\n",
    "    'himself', 'herself',\n",
    "    'man', 'woman',\n",
    "    'men', 'women',\n",
    "    'male', 'female',\n",
    "    'boy', 'girl',\n",
    "    'boys', 'girls',\n",
    "    'father', 'mother',\n",
    "    'fathers', 'mothers',\n",
    "    'son', 'daughter',\n",
    "    'sons', 'daughters',\n",
    "    'brother', 'sister',\n",
    "    'brothers', 'sisters'\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    compact_after_ingest=False,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "## **Step 3: Pivot to Yearly Indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:34:30.978547Z",
     "start_time": "2025-12-25T06:16:25.748406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-06 20:37:14\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Target DB:            ...k202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       ...ch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 30 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     4.11M            0.0x          570·30·0        198.5k/s          21s       \n",
      "     12.56M           0.0x          570·30·0        409.3k/s          31s       \n",
      "     18.26M           0.0x          570·30·0        448.9k/s          41s       \n",
      "     18.60M           0.0x          570·30·0        366.8k/s          51s       \n",
      "     18.84M           0.0x          570·30·0        310.4k/s         1m00s      \n",
      "     20.01M           7.7x          569·30·1        283.1k/s         1m10s      \n",
      "     29.92M          34.2x         557·30·13        370.7k/s         1m20s      \n",
      "     40.08M          40.3x         549·30·21        441.9k/s         1m30s      \n",
      "     51.04M          39.1x         542·30·28        506.8k/s         1m40s      \n",
      "     56.24M          36.2x         541·30·29        508.1k/s         1m50s      \n",
      "     58.70M          34.6x         541·30·29        486.4k/s         2m00s      \n",
      "     59.32M          34.3x         541·30·29        453.8k/s         2m10s      \n",
      "     63.26M          38.4x         536·30·34        449.5k/s         2m20s      \n",
      "     68.91M          41.9x         531·30·39        457.3k/s         2m30s      \n",
      "     77.58M          42.2x         524·30·46        482.8k/s         2m40s      \n",
      "     83.51M          43.2x         521·30·49        489.2k/s         2m50s      \n",
      "     91.24M          43.4x         515·30·55        504.9k/s         3m00s      \n",
      "     96.04M          43.0x         513·30·57        503.6k/s         3m10s      \n",
      "     99.59M          42.7x         512·30·58        496.2k/s         3m20s      \n",
      "    102.97M          44.4x         509·30·61        488.8k/s         3m30s      \n",
      "    110.07M          45.3x         502·30·68        498.7k/s         3m40s      \n",
      "    116.53M          45.6x         498·30·72        505.1k/s         3m50s      \n",
      "    123.35M          45.9x         493·30·77        512.4k/s         4m00s      \n",
      "    127.99M          46.4x         489·30·81        510.5k/s         4m10s      \n",
      "    136.71M          48.1x         477·30·93        524.4k/s         4m20s      \n",
      "    141.59M          47.3x         472·30·98        523.1k/s         4m30s      \n",
      "    149.15M          48.8x         460·30·110       531.4k/s         4m40s      \n",
      "    161.58M          49.1x         445·30·125       555.9k/s         4m50s      \n",
      "    166.24M          48.7x         441·30·129       552.8k/s         5m00s      \n",
      "    174.95M          48.7x         426·30·144       563.0k/s         5m10s      \n",
      "    184.13M          48.3x         413·30·157       574.1k/s         5m20s      \n",
      "    191.00M          48.1x         403·30·167       577.6k/s         5m30s      \n",
      "    198.62M          48.1x         389·30·181       583.0k/s         5m40s      \n",
      "    206.11M          48.5x         371·30·199       587.7k/s         5m50s      \n",
      "    213.64M          48.6x         351·30·219       592.3k/s         6m00s      \n",
      "    222.10M          48.3x         329·30·241       599.1k/s         6m10s      \n",
      "    230.86M          48.0x         307·30·263       606.4k/s         6m20s      \n",
      "    240.34M          47.3x         286·30·284       615.1k/s         6m30s      \n",
      "    248.01M          46.5x         275·30·295       618.9k/s         6m40s      \n",
      "    251.92M          46.5x         270·30·300       613.4k/s         6m50s      \n",
      "    263.81M          46.6x         250·30·320       627.1k/s         7m00s      \n",
      "    267.74M          46.2x         248·30·322       621.7k/s         7m10s      \n",
      "    276.07M          46.6x         232·30·338       626.4k/s         7m20s      \n",
      "    284.65M          46.2x         221·30·349       631.6k/s         7m30s      \n",
      "    290.52M          46.5x         212·30·358       630.6k/s         7m40s      \n",
      "    302.88M          46.2x         193·30·377       643.5k/s         7m50s      \n",
      "    305.75M          46.1x         191·30·379       636.1k/s         8m00s      \n",
      "    317.67M          46.4x         165·30·405       647.4k/s         8m10s      \n",
      "    323.01M          45.9x         162·30·408       645.1k/s         8m20s      \n",
      "    331.64M          46.3x         140·30·430       649.4k/s         8m30s      \n",
      "    338.02M          45.9x         133·30·437       649.2k/s         8m40s      \n",
      "    346.87M          46.1x         112·30·458       653.6k/s         8m50s      \n",
      "    354.07M          45.9x         98·30·472        654.8k/s         9m00s      \n",
      "    360.45M          46.4x          0·22·578        654.5k/s         9m10s      \n",
      "    365.88M          46.7x          0·4·596         652.5k/s         9m20s      \n",
      "    367.07M          46.7x          0·1·599         643.2k/s         9m30s      \n",
      "    367.27M          46.7x          0·1·599         632.5k/s         9m40s      \n",
      "    367.50M          46.7x          0·1·599         622.2k/s         9m50s      \n",
      "    367.50M          46.7x          0·1·599         611.8k/s         10m00s     \n",
      "    367.50M          46.7x          0·1·599         601.8k/s         10m10s     \n",
      "    367.50M          46.7x          0·1·599         592.1k/s         10m20s     \n",
      "    367.50M          46.7x          0·1·599         582.7k/s         10m30s     \n",
      "    368.20M          46.8x          0·0·600         574.7k/s         10m40s     \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    368.20M          46.8x          0·0·600         566.5k/s         10m49s     \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:36<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 8,614,673,680 (estimated)                                                                 │\n",
      "│ Size: 125.08 GB                                                                                  │\n",
      "│ Database: ...tch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    compact_after_ingest=False,\n",
    "    progress_every_s=10.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:35:13.418972Z",
     "start_time": "2025-12-25T06:35:03.313440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1472] <UNK> <UNK> <UNK> <UNK> absence\n",
      "     Value: 5 occurrences in 3 documents\n",
      "\n",
      "[ 2] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordance\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordingly\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accrue\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1472] <UNK> <UNK> <UNK> <UNK> acre\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3519e6ee50c4109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:06.913551Z",
     "start_time": "2025-12-25T06:39:56.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007e368657273656c66203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] herself <UNK> <UNK> <UNK> <UNK>\n",
      "     Value: 15,021 occurrences in 14,857 documents\n",
      "\n",
      "[ 2] Key:   [2019] herself <UNK> <UNK> <UNK> able\n",
      "     Value: 53 occurrences in 51 documents\n",
      "\n",
      "[ 3] Key:   [2019] herself <UNK> <UNK> <UNK> absence\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [2019] herself <UNK> <UNK> <UNK> absurd\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [2019] herself <UNK> <UNK> <UNK> abyss\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] herself <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63e70a0ce04fb",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:20.607162Z",
     "start_time": "2025-12-25T06:40:10.425620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007763c554e4b3e2068696d73656c66:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1910] <UNK> himself <UNK> <UNK> <UNK>\n",
      "     Value: 94,539 occurrences in 90,099 documents\n",
      "\n",
      "[ 2] Key:   [1910] <UNK> himself <UNK> <UNK> ability\n",
      "     Value: 35 occurrences in 29 documents\n",
      "\n",
      "[ 3] Key:   [1910] <UNK> himself <UNK> <UNK> abject\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 4] Key:   [1910] <UNK> himself <UNK> <UNK> able\n",
      "     Value: 99 occurrences in 99 documents\n",
      "\n",
      "[ 5] Key:   [1910] <UNK> himself <UNK> <UNK> aboriginal\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[1910] <UNK> himself\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a850b70859d225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexichron)",
   "language": "python",
   "name": "lexichron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
