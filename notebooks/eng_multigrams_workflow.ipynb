{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a1e3e9b344be",
   "metadata": {},
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd936c0a392494",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f261bd4a6317873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.700012Z",
     "start_time": "2026-01-02T00:15:41.111800Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramprep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramprep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramprep.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramprep.utilities.peek import db_head, db_peek, db_peek_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5328f85c059eda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.870970Z",
     "start_time": "2026-01-02T00:15:41.713856Z"
    }
   },
   "outputs": [],
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = None\n",
    "release = '20200217'\n",
    "language = 'eng-us'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996606b00401532",
   "metadata": {},
   "source": [
    "## **Step 1: Download and Ingest**\n",
    "\n",
    "Specifying `combined_bigrams_download` will convert compound terms to single, hyphenated tokens. This process is case-sensitive, so we specify all common capitalization patterns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:50:58.847936Z",
     "start_time": "2025-12-24T20:09:09.627237Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Try' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m      1\u001b[39m combined_bigrams_download = {\n\u001b[32m      2\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mworking class\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWorking class\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWorking Class\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mworking classes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWorking classes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mWorking Classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      3\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmiddle class\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMiddle class\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMiddle Class\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmiddle classes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMiddle classes\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMiddle Classes\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m      6\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhuman being\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHuman being\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHuman Being\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhuman beings\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHuman beings\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mHuman Beings\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m }\n\u001b[32m      9\u001b[39m download_and_ingest_to_rocksdb(\n\u001b[32m     10\u001b[39m     ngram_size=ngram_size,\n\u001b[32m     11\u001b[39m     repo_release_id=release,\n\u001b[32m     12\u001b[39m     repo_corpus_id=language,\n\u001b[32m     13\u001b[39m     db_path_stub=db_path_stub,\n\u001b[32m     14\u001b[39m     archive_path_stub=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     15\u001b[39m     ngram_type=\u001b[33m\"\u001b[39m\u001b[33mtagged\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     16\u001b[39m     random_seed=\u001b[32m289\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     overwrite_db=\u001b[43mTry\u001b[49m,\n\u001b[32m     18\u001b[39m     workers=\u001b[32m128\u001b[39m,\n\u001b[32m     19\u001b[39m     write_batch_size=\u001b[32m5_000\u001b[39m,\n\u001b[32m     20\u001b[39m     open_type=\u001b[33m\"\u001b[39m\u001b[33mwrite:packed24\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     21\u001b[39m     compact_after_ingest=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     22\u001b[39m     combined_bigrams=combined_bigrams_download\n\u001b[32m     23\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'Try' is not defined"
     ]
    }
   ],
   "source": [
    "combined_bigrams_download = {\n",
    "    \"working class\", \"Working class\", \"Working Class\", \"working classes\", \"Working classes\", \"Working Classes\"\n",
    "    \"middle class\", \"Middle class\", \"Middle Class\", \"middle classes\", \"Middle classes\", \"Middle Classes\"\n",
    "    \"lower class\", \"Lower class\", \"Lower Class\", \"lower classes\", \"Lower classes\", \"Lower Classes\"\n",
    "    \"upper class\", \"Upper class\", \"Upper Class\", \"upper classes\", \"Upper classes\", \"Upper Classes\"\n",
    "    \"human being\", \"Human being\", \"Human Being\", \"human beings\", \"Human beings\", \"Human Beings\"\n",
    "}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=289,\n",
    "    overwrite_db=True,\n",
    "    workers=128,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=False,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:13:21.359494Z",
     "start_time": "2025-12-25T05:08:45.236033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-05 08:58:43\n",
      "Mode:       RESUME\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "Target DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Temp directory:       .../edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Filter Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Min token length:     disabled\n",
      "Stopword filtering:   enabled (no stopwords loaded)\n",
      "Lemmatization:        enabled (no lemmatizer loaded)\n",
      "Always include:       52 token(s)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...Google_Books//20200217/eng-us/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Resuming existing work units\n",
      "Resuming: 0 completed, 0 processing, 0 pending\n",
      "\n",
      "Phase 2: Processing 0 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "No pending work units - skipping processing phase\n",
      "\n",
      "Phase 3: Ingesting 0 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "All shards already ingested - skipping ingestion phase\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 267,412,626 (estimated)                                                                   │\n",
      "│ Size: 278.90 GB                                                                                  │\n",
      "│ Database: ...h/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    \"brad\", \"brendan\", \"geoffrey\", \"greg\", \"brett\", \"jay\", \"matthew\", \"neil\", \"todd\",\n",
    "    \"allison\", \"anne\", \"carrie\", \"emily\", \"jill\", \"laurie\", \"kristen\", \"meredith\", \"sarah\",\n",
    "    \"darnell\", \"hakim\", \"jermaine\", \"kareem\", \"jamal\", \"leroy\", \"rasheed\", \"tremayne\", \"tyrone\",\n",
    "    \"aisha\", \"ebony\", \"keisha\", \"kenya\", \"latonya\", \"lakisha\", \"latoya\", \"tamika\", \"tanisha\",\n",
    "    \"joy\", \"love\", \"peace\", \"wonderful\", \"pleasure\", \"friend\", \"laughter\", \"happy\",\n",
    "    \"agony\", \"terrible\", \"horrible\", \"nasty\", \"evil\", \"war\", \"awful\", \"failure\"\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"resume\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    compact_after_ingest=False,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "## **Step 3: Pivot to Yearly Indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:34:30.978547Z",
     "start_time": "2025-12-25T06:16:25.748406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-05 08:59:38\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Target DB:            ...k202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       ...ch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 30 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     3.64M            0.0x          570·30·0        174.0k/s          21s       \n",
      "     11.70M           0.0x          570·30·0        378.5k/s          31s       \n",
      "     17.47M           0.0x          570·30·0        426.8k/s          41s       \n",
      "     17.93M           0.0x          570·30·0        352.3k/s          51s       \n",
      "     17.93M           0.0x          570·30·0        294.3k/s         1m00s      \n",
      "     18.77M           7.8x          569·30·1        264.8k/s         1m10s      \n",
      "     25.42M          31.3x          561·30·9        314.3k/s         1m20s      \n",
      "     37.44M          42.9x         549·30·21        411.7k/s         1m30s      \n",
      "     48.43M          40.9x         542·30·28        480.0k/s         1m40s      \n",
      "     55.05M          37.4x         540·30·30        496.4k/s         1m50s      \n",
      "     57.59M          35.8x         540·30·30        476.2k/s         2m00s      \n",
      "     58.28M          35.3x         540·30·30        445.1k/s         2m10s      \n",
      "     60.70M          37.7x         537·30·33        430.8k/s         2m20s      \n",
      "     67.93M          44.5x         529·30·41        450.2k/s         2m30s      \n",
      "     78.28M          45.3x         520·30·50        486.4k/s         2m40s      \n",
      "     87.28M          44.0x         515·30·55        510.6k/s         2m50s      \n",
      "     93.54M          43.2x         512·30·58        517.0k/s         3m00s      \n",
      "     96.15M          42.8x         511·30·59        503.6k/s         3m10s      \n",
      "     98.10M          42.4x         510·30·60        488.2k/s         3m20s      \n",
      "    101.37M          44.7x         507·30·63        480.6k/s         3m30s      \n",
      "    109.92M          47.1x         498·30·72        497.6k/s         3m40s      \n",
      "    120.79M          46.9x         489·30·81        523.1k/s         3m50s      \n",
      "    127.37M          47.1x         483·30·87        528.7k/s         4m00s      \n",
      "    135.24M          47.3x         472·30·98        539.0k/s         4m10s      \n",
      "    140.02M          47.4x         465·30·105       536.7k/s         4m20s      \n",
      "    147.48M          48.1x         454·30·116       544.4k/s         4m30s      \n",
      "    157.06M          48.5x         443·30·127       559.1k/s         4m40s      \n",
      "    164.91M          48.5x         431·30·139       566.8k/s         4m50s      \n",
      "    174.38M          47.9x         416·30·154       579.5k/s         5m00s      \n",
      "    182.23M          47.7x         404·30·166       586.1k/s         5m10s      \n",
      "    189.43M          47.8x         391·30·179       590.2k/s         5m20s      \n",
      "    196.50M          48.2x         375·30·195       593.8k/s         5m30s      \n",
      "    204.75M          48.3x         351·30·219       600.6k/s         5m40s      \n",
      "    210.78M          48.2x         334·30·236       600.7k/s         5m50s      \n",
      "    219.48M          48.1x         306·30·264       608.1k/s         6m00s      \n",
      "    229.79M          47.4x         281·30·289       619.5k/s         6m10s      \n",
      "    237.35M          46.4x         272·30·298       623.1k/s         6m20s      \n",
      "    242.60M          46.9x         260·30·310       620.6k/s         6m30s      \n",
      "    253.93M          46.5x         244·30·326       633.4k/s         6m40s      \n",
      "    257.47M          46.4x         239·30·331       626.5k/s         6m50s      \n",
      "    268.12M          46.6x         220·30·350       637.0k/s         7m00s      \n",
      "    274.84M          46.2x         212·30·358       637.8k/s         7m10s      \n",
      "    283.43M          46.4x         194·30·376       642.8k/s         7m20s      \n",
      "    292.53M          46.0x         183·30·387       648.7k/s         7m30s      \n",
      "    300.07M          46.2x         166·30·404       651.0k/s         7m40s      \n",
      "    308.87M          45.9x         152·30·418       655.9k/s         7m50s      \n",
      "    316.86M          46.1x         132·30·438       658.9k/s         8m00s      \n",
      "    323.86M          45.8x         122·30·448       659.7k/s         8m10s      \n",
      "    331.82M          46.2x         102·30·468       662.4k/s         8m20s      \n",
      "    338.92M          45.9x          0·27·573        663.3k/s         8m30s      \n",
      "    342.94M          46.3x          0·6·594         658.3k/s         8m40s      \n",
      "    344.51M          46.4x          0·1·599         648.9k/s         8m50s      \n",
      "    344.72M          46.3x          0·1·599         637.3k/s         9m00s      \n",
      "    344.95M          46.3x          0·1·599         626.2k/s         9m10s      \n",
      "    345.07M          46.3x          0·1·599         615.2k/s         9m20s      \n",
      "    345.07M          46.3x          0·1·599         604.4k/s         9m30s      \n",
      "    345.07M          46.3x          0·1·599         594.0k/s         9m40s      \n",
      "    345.07M          46.3x          0·1·599         583.9k/s         9m50s      \n",
      "    345.07M          46.3x          0·1·599         574.2k/s         10m00s     \n",
      "    345.07M          46.3x          0·1·599         564.8k/s         10m10s     \n",
      "    345.80M          46.4x          0·0·600         556.9k/s         10m20s     \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    345.80M          46.4x          0·0·600         550.2k/s         10m28s     \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:35<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 8,028,458,847 (estimated)                                                                 │\n",
      "│ Size: 117.19 GB                                                                                  │\n",
      "│ Database: ...tch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    compact_after_ingest=False,\n",
    "    progress_every_s=10.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:35:13.418972Z",
     "start_time": "2025-12-25T06:35:03.313440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1472] <UNK> <UNK> <UNK> <UNK> absence\n",
      "     Value: 5 occurrences in 3 documents\n",
      "\n",
      "[ 2] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordance\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordingly\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accrue\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1472] <UNK> <UNK> <UNK> <UNK> acre\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3519e6ee50c4109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:06.913551Z",
     "start_time": "2025-12-25T06:39:56.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007e347656f6666726579203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] ab <UNK> <UNK> <UNK> <UNK>\n",
      "     Value: 1,029 occurrences in 793 documents\n",
      "\n",
      "[ 2] Key:   [2019] ab <UNK> <UNK> <UNK> ab\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 3] Key:   [2019] ab <UNK> <UNK> <UNK> ad\n",
      "     Value: 37 occurrences in 27 documents\n",
      "\n",
      "[ 4] Key:   [2019] ab <UNK> <UNK> <UNK> adequate\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [2019] ab <UNK> <UNK> <UNK> also\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] Geoffrey <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63e70a0ce04fb",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:20.607162Z",
     "start_time": "2025-12-25T06:40:10.425620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007d63c554e4b3e2047656f6666726579:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "No keys found with prefix 000007d63c554e4b3e2047656f6666726579\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[2006] <UNK> Geoffrey\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a850b70859d225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexichron)",
   "language": "python",
   "name": "lexichron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
