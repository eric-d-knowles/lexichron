{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ],
   "id": "b76a1e3e9b344be"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "98dd936c0a392494"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T00:16:50.971585Z",
     "start_time": "2025-11-30T00:16:50.131118Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramkit.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramkit.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramkit.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramkit.utilities.peek import db_head, db_peek, db_peek_prefix"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T00:16:51.217615Z",
     "start_time": "2025-11-30T00:16:50.987541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = '/scratch/edk202/NLP_archive/Google_Books/'\n",
    "release = '20200217'\n",
    "language = 'eng'\n",
    "ngram_size = 5\n",
    "bin_size = 5"
   ],
   "id": "d5328f85c059eda4",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Step 1: Download and Ingest**",
   "id": "2996606b00401532"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-24T02:22:26.463849Z",
     "start_time": "2025-11-24T01:40:01.275708Z"
    }
   },
   "source": [
    "combined_bigrams_download = {\"lower class\", \"working class\", \"middle class\", \"upper class\", \"blue collar\", \"white collar\", \"african american\", \"african americans\", \"european american\", \"european americans\", \"white people\", \"white person\", \"white americans\", \"black american\", \"black americans\", \"black person\", \"black people\", \"human being\"}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=98,\n",
    "    overwrite_db=False,\n",
    "    workers=80,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=True,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-23 20:40:01\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "File range:           0 to 19422\n",
      "Total files:          19423\n",
      "Files to get:         0\n",
      "Skipping:             19423\n",
      "Download workers:     80\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         False\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed:   0%|                                                               | 0/0 [00:00<?]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         2.21 TB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compaction completed in 0:42:07\n",
      "Size before:             2.21 TB\n",
      "Size after:              2.21 TB\n",
      "Space saved:             -143.08 KB (-0.0%)\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       0\n",
      "Failed files:                0\n",
      "Total entries written:       0\n",
      "Write batches flushed:       0\n",
      "Uncompressed data processed: 0.00 B\n",
      "Processing throughput:       0.00 MB/sec\n",
      "\n",
      "End Time: 2025-11-23 21:22:26.459660\n",
      "Total Runtime: 0:42:25.150204\n",
      "Time per file: 0:00:00\n",
      "Files per hour: 0.0\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T01:20:54.539141Z",
     "start_time": "2025-11-30T00:16:52.945557Z"
    }
   },
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\"lower-class\", \"working-class\", \"middle-class\", \"upper-class\", \"blue-collar\", \"white-collar\", \"african-american\", \"african-americans\", \"european-american\", \"european-americans\", \"white-people\", \"white-person\", \"white-americans\", \"black-american\", \"black-americans\", \"black-person\", \"black-people\", \"human-being\"}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-29 19:16:52\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams.db\n",
      "Target DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Temp directory:       ...tch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             5 (5-year bins)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...ra/Google_Books//20200217/eng/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    12.42M        75.6%         36/36       534·36·30      408.9k/s        30s      \n",
      "    56.21M        79.5%         36/36       522·36·42      930.5k/s       1m00s     \n",
      "   101.50M        80.5%         36/36       506·36·58     1122.4k/s       1m30s     \n",
      "   144.45M        81.0%         36/36       496·36·68     1199.7k/s       2m00s     \n",
      "   189.70M        81.0%         36/36       485·36·79     1261.4k/s       2m30s     \n",
      "   233.58M        81.3%         36/36       475·36·89     1294.8k/s       3m00s     \n",
      "   276.92M        82.0%         36/36       474·36·90     1316.0k/s       3m30s     \n",
      "   321.03M        82.5%         36/36       469·36·95     1335.5k/s       4m00s     \n",
      "   364.57M        82.7%         36/36       467·36·97     1348.1k/s       4m30s     \n",
      "   406.74M        83.0%         36/36       464·36·100    1353.8k/s       5m00s     \n",
      "   448.05M        83.2%         36/36       455·36·109    1356.1k/s       5m30s     \n",
      "   488.79M        83.5%         36/36       447·36·117    1356.2k/s       6m00s     \n",
      "   532.68M        83.5%         36/36       438·36·126    1364.5k/s       6m30s     \n",
      "   574.48M        83.6%         36/36       431·36·133    1366.5k/s       7m00s     \n",
      "   617.44M        83.6%         36/36       416·36·148    1370.9k/s       7m30s     \n",
      "   656.59M        83.5%         36/36       406·36·158    1366.7k/s       8m00s     \n",
      "   701.89M        83.5%         36/36       396·36·168    1375.2k/s       8m30s     \n",
      "   744.47M        83.4%         36/36       389·36·175    1377.6k/s       9m00s     \n",
      "   787.10M        83.6%         36/36       380·36·184    1379.8k/s       9m30s     \n",
      "   828.18M        83.8%         36/36       365·36·199    1379.4k/s       10m00s    \n",
      "   866.13M        83.9%         36/36       346·36·218    1373.9k/s       10m30s    \n",
      "   906.93M        84.0%         36/36       335·36·229    1373.2k/s       11m00s    \n",
      "   948.65M        84.0%         36/36       324·36·240    1374.1k/s       11m30s    \n",
      "   991.19M        84.1%         36/36       315·36·249    1375.9k/s       12m00s    \n",
      "    1.03B         84.1%         36/36       303·36·261    1377.1k/s       12m30s    \n",
      "    1.08B         84.0%         36/36       295·36·269    1380.5k/s       13m00s    \n",
      "    1.12B         84.0%         36/36       283·36·281    1381.0k/s       13m30s    \n",
      "    1.16B         83.9%         36/36       272·36·292    1381.3k/s       14m00s    \n",
      "    1.21B         83.9%         36/36       252·36·312    1384.4k/s       14m30s    \n",
      "    1.24B         83.9%         36/36       239·36·325    1381.8k/s       15m00s    \n",
      "    1.29B         83.8%         36/36       235·35·330    1387.6k/s       15m30s    \n",
      "    1.34B         83.8%         36/36       228·36·336    1390.1k/s       16m00s    \n",
      "    1.38B         83.8%         36/36       222·36·342    1391.3k/s       16m30s    \n",
      "    1.42B         83.8%         36/36       210·36·354    1391.3k/s       17m00s    \n",
      "    1.46B         83.9%         36/36       207·36·357    1394.3k/s       17m30s    \n",
      "    1.51B         83.9%         36/36       197·36·367    1393.1k/s       18m00s    \n",
      "    1.55B         84.1%         36/36       189·36·375    1392.2k/s       18m30s    \n",
      "    1.59B         84.2%         36/36       181·36·383    1391.5k/s       19m00s    \n",
      "    1.63B         84.2%         36/36       166·36·398    1391.9k/s       19m30s    \n",
      "    1.67B         84.1%         36/36       156·36·408    1393.3k/s       20m00s    \n",
      "    1.72B         84.0%         36/36       143·36·421    1395.0k/s       20m30s    \n",
      "    1.76B         83.8%         36/36       132·36·432    1399.6k/s       21m00s    \n",
      "    1.81B         83.8%         36/36       112·36·452    1399.6k/s       21m30s    \n",
      "    1.85B         83.8%         36/36       102·36·462    1402.1k/s       22m00s    \n",
      "    1.89B         83.8%         36/36       96·36·468     1403.2k/s       22m30s    \n",
      "    1.94B         83.7%         36/36       84·36·480     1404.6k/s       23m00s    \n",
      "    1.98B         83.8%         36/36       73·36·491     1404.0k/s       23m30s    \n",
      "    2.02B         83.8%         36/36       66·36·498     1405.8k/s       24m00s    \n",
      "    2.06B         83.8%         36/36       52·36·512     1404.3k/s       24m30s    \n",
      "    2.11B         83.9%         36/36       46·36·518     1404.5k/s       25m00s    \n",
      "    2.15B         83.9%         36/36       35·36·529     1406.6k/s       25m30s    \n",
      "    2.19B         83.9%         36/36       27·36·537     1406.1k/s       26m00s    \n",
      "    2.24B         83.9%         36/36       20·36·544     1406.9k/s       26m30s    \n",
      "    2.28B         83.9%         36/36        3·36·561     1406.8k/s       27m00s    \n",
      "    2.32B         83.9%         29/36        0·29·571     1408.1k/s       27m30s    \n",
      "    2.36B         83.9%         22/36        0·22·578     1402.4k/s       28m00s    \n",
      "    2.39B         83.9%         21/36        0·21·579     1395.9k/s       28m30s    \n",
      "    2.41B         83.9%         15/36        0·15·585     1386.9k/s       29m00s    \n",
      "    2.43B         84.0%         13/36        0·13·587     1374.5k/s       29m30s    \n",
      "    2.45B         84.0%          9/36        0·9·591      1360.3k/s       30m00s    \n",
      "    2.46B         84.0%          7/36        0·7·593      1344.3k/s       30m30s    \n",
      "    2.47B         84.1%          4/36        0·4·596      1326.6k/s       31m00s    \n",
      "    2.47B         84.1%          4/36        0·4·596      1308.5k/s       31m30s    \n",
      "    2.48B         84.1%          2/36        0·2·598      1289.8k/s       32m00s    \n",
      "    2.48B         84.1%          1/36        0·1·599      1271.3k/s       32m30s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    2.48B         84.1%          0/36        0·0·600      1255.3k/s       32m55s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [26:29<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 501,124,594 items in 1590.0s (315,176 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         181.05 GB\n",
      "Compaction completed in 0:04:22\n",
      "Size before:             181.05 GB\n",
      "Size after:              115.49 GB\n",
      "Space saved:             65.56 GB (36.2%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 438,496,183 (estimated)                                                                   │\n",
      "│ Size: 115.49 GB                                                                                  │\n",
      "│ Database: ...atch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": "## **Step 3: Pivot to Yearly Indices**"
  },
  {
   "cell_type": "code",
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T01:39:11.296684Z",
     "start_time": "2025-11-30T01:21:33.675336Z"
    }
   },
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=40,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    ");"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-11-29 20:21:33\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...dk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_processed.db\n",
      "Target DB:            .../edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              40\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: True\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 40 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     34.62M           0.0x          560·40·0       1114.6k/s          31s       \n",
      "    115.22M          13.9x         520·40·40       1886.5k/s         1m01s      \n",
      "    159.79M          16.6x         495·40·65       1754.0k/s         1m31s      \n",
      "    221.44M          17.1x         455·40·105      1828.8k/s         2m01s      \n",
      "    296.92M          17.5x         388·40·172      1965.1k/s         2m31s      \n",
      "    364.89M          17.7x         273·40·287      2015.2k/s         3m01s      \n",
      "    427.65M          17.3x         219·40·341      2025.9k/s         3m31s      \n",
      "    493.01M          17.5x         140·40·420      2045.2k/s         4m01s      \n",
      "    556.73M          17.5x          0·2·598        2053.7k/s         4m31s      \n",
      "    557.78M          17.5x          0·1·599        1852.4k/s         5m01s      \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    559.12M          17.5x          0·0·600        1777.2k/s         5m14s      \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:29<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "Post-Ingestion Compaction\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Initial DB size:         72.78 GB\n",
      "Compaction completed in 0:11:46\n",
      "Size before:             72.78 GB\n",
      "Size after:              172.46 GB\n",
      "Space saved:             -99.68 GB (-137.0%)\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 4,896,078,077 (estimated)                                                                 │\n",
      "│ Size: 172.46 GB                                                                                  │\n",
      "│ Database: /scratch/edk202/NLP_corpora/Google_Books/20200217/eng/5gram_files/5grams_pivoted.db    │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T01:40:17.643919Z",
     "start_time": "2025-11-30T01:40:16.994706Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1470] <UNK> <UNK> <UNK> <UNK> absence\n",
      "     Value: 5 occurrences in 3 documents\n",
      "\n",
      "[ 2] Key:   [1470] <UNK> <UNK> <UNK> <UNK> accordance\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1470] <UNK> <UNK> <UNK> <UNK> accordingly\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1470] <UNK> <UNK> <UNK> <UNK> accrue\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1470] <UNK> <UNK> <UNK> <UNK> acre\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_peek",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T01:53:05.146568Z",
     "start_time": "2025-11-30T01:53:04.397083Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2015] working-class <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007df776f726b696e672d636c617373203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2015] working-class <UNK> <UNK> ability\n",
      "     Value: 13 occurrences in 13 documents\n",
      "\n",
      "[ 2] Key:   [2015] working-class <UNK> <UNK> able\n",
      "     Value: 120 occurrences in 117 documents\n",
      "\n",
      "[ 3] Key:   [2015] working-class <UNK> <UNK> abolition\n",
      "     Value: 45 occurrences in 42 documents\n",
      "\n",
      "[ 4] Key:   [2015] working-class <UNK> <UNK> abortive\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 5] Key:   [2015] working-class <UNK> <UNK> absence\n",
      "     Value: 13 occurrences in 13 documents\n",
      "\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "inspect_prefix_header",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T01:53:28.433416Z",
     "start_time": "2025-11-30T01:53:27.748306Z"
    }
   },
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[2000] <UNK> working-class\", n=5)"
   ],
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007d03c554e4b3e20776f726b696e672d636c617373:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2000] <UNK> working-class <UNK> <UNK>\n",
      "     Value: 138,742 occurrences in 128,220 documents\n",
      "\n",
      "[ 2] Key:   [2000] <UNK> working-class <UNK> ability\n",
      "     Value: 3 occurrences in 3 documents\n",
      "\n",
      "[ 3] Key:   [2000] <UNK> working-class <UNK> able\n",
      "     Value: 108 occurrences in 107 documents\n",
      "\n",
      "[ 4] Key:   [2000] <UNK> working-class <UNK> absent\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n",
      "[ 5] Key:   [2000] <UNK> working-class <UNK> absolutely\n",
      "     Value: 5 occurrences in 5 documents\n",
      "\n"
     ]
    }
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "24a850b70859d225",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
