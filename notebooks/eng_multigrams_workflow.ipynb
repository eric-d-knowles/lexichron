{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b76a1e3e9b344be",
   "metadata": {},
   "source": [
    "# **Multigrams: Full Workflow**\n",
    "The multigram workflow mirrors the unigram workflow, but with two differences. First, instead of _creating_ a whitelist, you filter the multigram corpus _using_ a whitelist containing the top-_N_ unigrams. Second, the multigram workflow adds a **pivoting** step. Pivoting reorganizes the database so that it's easy to query year-ngram combinations. For instance, you can learn how many times the word \"nuclear\" appeared in 2011 by querying the key `[2011] nuclear`. This is useful for analyzing changes in word meanings over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd936c0a392494",
   "metadata": {},
   "source": [
    "## **Setup**\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f261bd4a6317873c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.700012Z",
     "start_time": "2026-01-02T00:15:41.111800Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading en_core_web_sm...\n",
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Downloading zh_core_web_sm...\n",
      "Collecting zh-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.8.0/zh_core_web_sm-3.8.0-py3-none-any.whl (48.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.5/48.5 MB\u001b[0m \u001b[31m135.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-pkuseg<2.0.0,>=1.0.0 (from zh-core-web-sm==3.8.0)\n",
      "  Using cached spacy_pkuseg-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.19.0 in /scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages (from spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (1.26.4)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.3.0 in /scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages (from spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (2.5.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.3 in /scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages (from srsly<3.0.0,>=2.3.0->spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (2.0.10)\n",
      "Requirement already satisfied: cloudpickle>=2.2.0 in /scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages (from srsly<3.0.0,>=2.3.0->spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (3.1.2)\n",
      "Requirement already satisfied: ujson>=1.35 in /scratch/edk202/.conda/envs/lexichron/lib/python3.11/site-packages (from srsly<3.0.0,>=2.3.0->spacy-pkuseg<2.0.0,>=1.0.0->zh-core-web-sm==3.8.0) (5.11.0)\n",
      "Using cached spacy_pkuseg-1.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.2 MB)\n",
      "Installing collected packages: spacy-pkuseg, zh-core-web-sm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [zh-core-web-sm]m [zh-core-web-sm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed spacy-pkuseg-1.0.1 zh-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('zh_core_web_sm')\n",
      "Downloading fr_core_news_sm...\n",
      "Collecting fr-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/fr_core_news_sm-3.8.0/fr_core_news_sm-3.8.0-py3-none-any.whl (16.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m123.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fr-core-news-sm\n",
      "Successfully installed fr-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('fr_core_news_sm')\n",
      "Downloading de_core_news_sm...\n",
      "Collecting de-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-3.8.0/de_core_news_sm-3.8.0-py3-none-any.whl (14.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.6/14.6 MB\u001b[0m \u001b[31m100.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: de-core-news-sm\n",
      "Successfully installed de-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n",
      "Downloading it_core_news_sm...\n",
      "Collecting it-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m118.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
      "Successfully installed it-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('it_core_news_sm')\n",
      "Downloading ru_core_news_sm...\n",
      "Collecting ru-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m117.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
      "  Using cached pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
      "  Using cached dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
      "  Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
      "Using cached pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
      "Using cached dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
      "Using cached pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
      "Installing collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4/4\u001b[0m [ru-core-news-sm] [ru-core-news-sm]\n",
      "\u001b[1A\u001b[2KSuccessfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('ru_core_news_sm')\n",
      "Downloading es_core_news_sm...\n",
      "Collecting es-core-news-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.8.0/es_core_news_sm-3.8.0-py3-none-any.whl (12.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m61.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0mm0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from ngramprep.ngram_acquire import download_and_ingest_to_rocksdb\n",
    "from ngramprep.ngram_filter.pipeline.orchestrator import build_processed_db\n",
    "from ngramprep.ngram_pivot.pipeline import build_pivoted_db\n",
    "from ngramprep.utilities.peek import db_head, db_peek, db_peek_prefix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": [
    "### Configure\n",
    "Here we set basic parameters: the corpus to download, the size of the ngrams to download, and the size of the year bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5328f85c059eda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-02T00:15:41.870970Z",
     "start_time": "2026-01-02T00:15:41.713856Z"
    }
   },
   "outputs": [],
   "source": [
    "db_path_stub = '/scratch/edk202/NLP_corpora/Google_Books/'\n",
    "archive_path_stub = None\n",
    "release = '20200217'\n",
    "language = 'eng-us'\n",
    "ngram_size = 5\n",
    "bin_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2996606b00401532",
   "metadata": {},
   "source": [
    "## **Step 1: Download and Ingest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-24T20:50:58.847936Z",
     "start_time": "2025-12-24T20:09:09.627237Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-GRAM ACQUISITION PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-03 19:17:22\n",
      "\n",
      "Download Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Ngram repo:           https://books.storage.googleapis.com/?prefix=ngrams/books/20200217/eng-us/5-\n",
      "DB path:              /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "File range:           0 to 11144\n",
      "Total files:          11145\n",
      "Files to get:         0\n",
      "Skipping:             11145\n",
      "Download workers:     100\n",
      "Batch size:           5,000\n",
      "Ngram size:           5\n",
      "Ngram type:           tagged\n",
      "Overwrite DB:         False\n",
      "DB Profile:           write:packed24\n",
      "\n",
      "Download Progress\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed:   0%|                                                               | 0/0 [00:00<?]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Fully processed files:       0\n",
      "Failed files:                0\n",
      "Total entries written:       0\n",
      "Write batches flushed:       0\n",
      "Uncompressed data processed: 0.00 B\n",
      "Processing throughput:       0.00 MB/sec\n",
      "\n",
      "End Time: 2026-01-03 19:18:03.544998\n",
      "Total Runtime: 0:00:40.683806\n",
      "Time per file: 0:00:00\n",
      "Files per hour: 0.0\n"
     ]
    }
   ],
   "source": [
    "combined_bigrams_download = {\n",
    "    \"working class\", \"working classes\",\n",
    "    \"middle class\", \"middle classes\",\n",
    "    \"lower class\", \"lower classes\",\n",
    "    \"upper class\", \"upper classes\",\n",
    "    \"human being\", \"human beings\"\n",
    "}\n",
    "\n",
    "download_and_ingest_to_rocksdb(\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    archive_path_stub=None,\n",
    "    ngram_type=\"tagged\",\n",
    "    random_seed=89,\n",
    "    overwrite_db=False,\n",
    "    workers=100,\n",
    "    write_batch_size=5_000,\n",
    "    open_type=\"write:packed24\",\n",
    "    compact_after_ingest=False,\n",
    "    combined_bigrams=combined_bigrams_download\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2_header",
   "metadata": {},
   "source": [
    "## **Step 2: Filter and Normalize**\n",
    "`config.py` contains generic defaults for the filtering pipeline. You can override these defaults by passing option dictionaries to the `build_processed_db` function, as seen below. As implemented here, we use the whitelist from the unigram workflow to filter the multigram corpus. If we weren't using a whitelist, we could normalize, filter, and lemmatize each token just as we did for the unigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "phase2_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:13:21.359494Z",
     "start_time": "2025-12-25T05:08:45.236033Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "N-GRAM FILTER PIPELINE\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-03 23:13:41\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams.db\n",
      "Target DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Temp directory:       .../edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/processing_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              36\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest readers:       4\n",
      "Queue size:           8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       10.0s\n",
      "\n",
      "Temporal Binning\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Bin size:             1 (annual data)\n",
      "\n",
      "Filter Options\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Min token length:     disabled\n",
      "Stopword filtering:   enabled (no stopwords loaded)\n",
      "Lemmatization:        enabled (no lemmatizer loaded)\n",
      "Always include:       10 token(s)\n",
      "\n",
      "Whitelist Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Input whitelist:      ...Google_Books//20200217/eng-us/1gram_files/1grams_processed.db/whitelist.txt\n",
      "  All tokens (min count: 1)\n",
      "Output whitelist:     None\n",
      "\n",
      "Loading whitelist...\n",
      "Loaded 30,000 tokens\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 36 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "    items         kept%        workers        units          rate        elapsed    \n",
      "────────────────────────────────────────────────────────────────────────────────────\n",
      "    4.32M         85.1%          4/36        594·4·2       105.6k/s        41s      \n",
      "    18.29M        81.9%          7/36        580·7·13      257.9k/s       1m10s     \n",
      "    40.13M        81.5%         10/36       567·10·23      397.4k/s       1m40s     \n",
      "    67.06M        82.7%         14/36       552·14·34      512.0k/s       2m10s     \n",
      "   109.57M        83.0%         17/36       535·17·48      680.7k/s       2m40s     \n",
      "   155.79M        82.8%         20/36       508·20·72      815.8k/s       3m10s     \n",
      "   210.56M        82.5%         23/36       465·23·112     952.9k/s       3m40s     \n",
      "   271.44M        83.6%         26/36       439·26·135    1081.5k/s       4m10s     \n",
      "   342.39M        84.3%         29/36       400·29·171    1218.6k/s       4m40s     \n",
      "   424.97M        84.7%         32/36       367·32·201    1366.5k/s       5m10s     \n",
      "   510.26M        84.8%         35/36       326·35·239    1496.6k/s       5m40s     \n",
      "   606.77M        85.0%         36/36       279·36·285    1635.7k/s       6m10s     \n",
      "   697.15M        85.2%         36/36       237·36·327    1738.6k/s       6m40s     \n",
      "   797.22M        84.9%         36/36       206·36·358    1849.8k/s       7m10s     \n",
      "   890.73M        84.9%         36/36       176·36·388    1932.3k/s       7m40s     \n",
      "   986.01M        84.8%         36/36       137·36·427    2008.4k/s       8m10s     \n",
      "    1.07B         84.5%         35/36       86·35·479     2060.6k/s       8m40s     \n",
      "    1.17B         84.5%         36/36       40·36·524     2128.4k/s       9m10s     \n",
      "    1.28B         84.5%         36/36       18·36·546     2195.9k/s       9m40s     \n",
      "    1.36B         84.5%         14/36        0·14·586     2220.5k/s       10m10s    \n",
      "    1.39B         84.7%          5/36        0·5·595      2167.5k/s       10m40s    \n",
      "───────────────────────────────────── final ─────────────────────────────────────\n",
      "    1.39B         84.7%          0/36        0·0·600      2077.1k/s       11m10s    \n",
      "\n",
      "Phase 3: Ingesting 600 shards with 4 parallel readers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Shards Ingested: 100%|███████████████████████████████████████████████████████| 600/600 [43:25<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingestion complete: 600 shards, 309,909,747 items in 2605.8s (118,932 items/s)\n",
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 267,449,339 (estimated)                                                                   │\n",
      "│ Size: 278.94 GB                                                                                  │\n",
      "│ Database: ...h/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "filter_options = {\n",
    "    'bin_size': bin_size\n",
    "}\n",
    "\n",
    "whitelist_options = {\n",
    "    'whitelist_path': f'{db_path_stub}/{release}/{language}/1gram_files/1grams_processed.db/whitelist.txt',\n",
    "    'output_whitelist_path': None\n",
    "}\n",
    "\n",
    "always_include_tokens = {\n",
    "    \"working-class\", \"working-classes\",\n",
    "    \"middle-class\", \"middle-classes\",\n",
    "    \"lower-class\", \"lower-classes\",\n",
    "    \"upper-class\", \"upper-classes\",\n",
    "    \"human-being\", \"human-beings\"\n",
    "}\n",
    "\n",
    "build_processed_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=36,\n",
    "    num_initial_work_units=600,\n",
    "    work_unit_claim_order=\"random\",\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    progress_every_s=30.0,\n",
    "    always_include=always_include_tokens,\n",
    "    compact_after_ingest=False,\n",
    "    **filter_options,\n",
    "    **whitelist_options\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3_header",
   "metadata": {},
   "source": [
    "## **Step 3: Pivot to Yearly Indices**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "phase3_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:34:30.978547Z",
     "start_time": "2025-12-25T06:16:25.748406Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PARALLEL N-GRAM DATABASE PIVOT\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2026-01-04 00:08:42\n",
      "Mode:       RESTART\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            ...02/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_processed.db\n",
      "Target DB:            ...k202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db\n",
      "Temp directory:       ...ch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/pivoting_tmp\n",
      "\n",
      "Parallelism\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Workers:              30\n",
      "Initial work units:   600\n",
      "\n",
      "Database Profiles\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Reader profile:       read:packed24\n",
      "Writer profile:       write:packed24\n",
      "Ingest profile:       write:packed24\n",
      "\n",
      "Ingestion Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Ingest mode:          direct_sst\n",
      "Ingest readers:       8\n",
      "Compact after ingest: False\n",
      "\n",
      "Worker Configuration\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "Flush interval:       5.0s\n",
      "\n",
      "\n",
      "Phase 1: Creating work units...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Clean restart - creating new work units\n",
      "Loading cached partitions (600 work units)...\n",
      "Loaded 600 work units from cache\n",
      "\n",
      "Phase 2: Processing 600 work units with 30 workers...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "     ngrams           exp            units            rate          elapsed     \n",
      "────────────────────────────────────────────────────────────────────────────────\n",
      "     5.19M            0.0x          570·30·0        234.9k/s          22s       \n",
      "     12.96M           0.0x          570·30·0        403.4k/s          32s       \n",
      "     17.52M           0.0x          570·30·0        416.2k/s          42s       \n",
      "     18.11M           0.0x          570·30·0        347.6k/s          52s       \n",
      "     18.11M           0.0x          570·30·0        291.7k/s         1m02s      \n",
      "     18.96M           7.8x          569·30·1        262.8k/s         1m12s      \n",
      "     26.89M          32.0x         559·30·11        327.4k/s         1m22s      \n",
      "     37.58M          42.8x         549·30·21        407.9k/s         1m32s      \n",
      "     47.94M          41.4x         542·30·28        469.4k/s         1m42s      \n",
      "     54.38M          37.9x         540·30·30        485.0k/s         1m52s      \n",
      "     57.18M          36.0x         540·30·30        468.3k/s         2m02s      \n",
      "     58.51M          35.2x         540·30·30        442.9k/s         2m12s      \n",
      "     60.28M          37.4x         538·30·32        424.2k/s         2m22s      \n",
      "     66.35M          41.3x         531·30·39        436.2k/s         2m32s      \n",
      "     75.79M          45.0x         522·30·48        467.5k/s         2m42s      \n",
      "     83.69M          43.3x         517·30·53        486.2k/s         2m52s      \n",
      "     89.64M          42.8x         514·30·56        492.2k/s         3m02s      \n",
      "     94.35M          43.7x         511·30·59        491.2k/s         3m12s      \n",
      "     96.71M          43.0x         510·30·60        478.5k/s         3m22s      \n",
      "     98.66M          44.2x         510·30·60        465.2k/s         3m32s      \n",
      "    104.93M          45.4x         503·30·67        472.4k/s         3m42s      \n",
      "    112.15M          46.4x         497·30·73        483.1k/s         3m52s      \n",
      "    121.22M          46.7x         489·30·81        500.7k/s         4m02s      \n",
      "    126.06M          46.8x         484·30·86        500.0k/s         4m12s      \n",
      "    133.10M          47.3x         474·30·96        507.8k/s         4m22s      \n",
      "    138.76M          47.8x         467·29·104       509.9k/s         4m32s      \n",
      "    146.81M          48.4x         454·30·116       520.4k/s         4m42s      \n",
      "    155.53M          48.5x         444·30·126       532.4k/s         4m52s      \n",
      "    164.23M          48.3x         432·30·138       543.5k/s         5m02s      \n",
      "    173.32M          48.1x         419·30·151       555.3k/s         5m12s      \n",
      "    180.04M          47.9x         407·30·163       558.9k/s         5m22s      \n",
      "    188.94M          47.8x         392·30·178       568.9k/s         5m32s      \n",
      "    195.06M          48.1x         378·30·192       570.2k/s         5m42s      \n",
      "    202.56M          48.4x         357·30·213       575.3k/s         5m52s      \n",
      "    209.81M          48.2x         335·30·235       579.4k/s         6m02s      \n",
      "    218.81M          48.1x         309·30·261       588.0k/s         6m12s      \n",
      "    228.96M          47.6x         281·30·289       599.2k/s         6m22s      \n",
      "    236.58M          46.6x         272·30·298       603.4k/s         6m32s      \n",
      "    241.25M          46.9x         264·30·306       600.0k/s         6m42s      \n",
      "    253.22M          46.5x         245·30·325       614.4k/s         6m52s      \n",
      "    257.12M          46.4x         240·30·330       609.1k/s         7m02s      \n",
      "    267.16M          46.7x         221·30·349       618.3k/s         7m12s      \n",
      "    274.88M          46.1x         212·30·358       621.8k/s         7m22s      \n",
      "    281.87M          46.4x         198·30·372       623.5k/s         7m32s      \n",
      "    292.19M          46.1x         183·30·387       632.3k/s         7m42s      \n",
      "    298.69M          46.3x         168·30·402       632.7k/s         7m52s      \n",
      "    308.47M          45.9x         153·30·417       639.8k/s         8m02s      \n",
      "    314.39M          46.2x         138·29·433       638.9k/s         8m12s      \n",
      "    323.80M          45.7x         122·30·448       644.9k/s         8m22s      \n",
      "    330.02M          46.1x         105·30·465       644.4k/s         8m32s      \n",
      "    338.26M          45.9x         19·29·552        647.9k/s         8m42s      \n",
      "    341.67M          46.3x          0·11·589        642.1k/s         8m52s      \n",
      "    344.25M          46.4x          0·2·598         635.0k/s         9m02s      \n",
      "    344.69M          46.3x          0·1·599         624.3k/s         9m12s      \n",
      "    344.88M          46.3x          0·1·599         613.5k/s         9m22s      \n",
      "    345.06M          46.3x          0·1·599         603.1k/s         9m32s      \n",
      "    345.16M          46.3x          0·1·599         592.9k/s         9m42s      \n",
      "    345.16M          46.3x          0·1·599         582.9k/s         9m52s      \n",
      "    345.16M          46.3x          0·1·599         573.3k/s         10m02s     \n",
      "    345.16M          46.3x          0·1·599         563.9k/s         10m12s     \n",
      "    345.17M          46.4x          0·1·599         554.8k/s         10m22s     \n",
      "───────────────────────────────────────────── final ────────────────────────────────────────────\n",
      "    345.85M          46.4x          0·0·600         547.6k/s         10m31s     \n",
      "\n",
      "Phase 3: Ingesting 600 SST files via direct ingestion...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST Files Ingested: 100%|████████████████████████████████████████████████████| 600/600 [00:37<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Phase 4: Finalizing database...\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "\n",
      "┌──────────────────────────────────────────────────────────────────────────────────────────────────┐\n",
      "│ PROCESSING COMPLETE                                                                              │\n",
      "├──────────────────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Items: 8,029,663,534 (estimated)                                                                 │\n",
      "│ Size: 117.21 GB                                                                                  │\n",
      "│ Database: ...tch/edk202/NLP_corpora/Google_Books/20200217/eng-us/5gram_files/5grams_pivoted.db   │\n",
      "└──────────────────────────────────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "build_pivoted_db(\n",
    "    mode=\"restart\",\n",
    "    ngram_size=ngram_size,\n",
    "    repo_release_id=release,\n",
    "    repo_corpus_id=language,\n",
    "    db_path_stub=db_path_stub,\n",
    "    num_workers=30,\n",
    "    num_initial_work_units=600,\n",
    "    cache_partitions=True,\n",
    "    use_cached_partitions=True,\n",
    "    compact_after_ingest=False,\n",
    "    progress_every_s=10.0,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": [
    "# Inspect Final Database\n",
    "Here are three functions you can use to inspect the final database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": [
    "## `db_head`: First N records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "inspect_head",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:35:13.418972Z",
     "start_time": "2025-12-25T06:35:03.313440Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1472] <UNK> <UNK> <UNK> <UNK> absence\n",
      "     Value: 5 occurrences in 3 documents\n",
      "\n",
      "[ 2] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordance\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 3] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accordingly\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [1472] <UNK> <UNK> <UNK> <UNK> accrue\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 5] Key:   [1472] <UNK> <UNK> <UNK> <UNK> acre\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_head(pivoted_db, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inspect_peek_header",
   "metadata": {},
   "source": [
    "## `db_peek`: Records starting from a key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c3519e6ee50c4109",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:06.913551Z",
     "start_time": "2025-12-25T06:39:56.810015Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007e3776f726b696e672d636c617373203c554e4b3e203c554e4b3e203c554e4b3e203c554e4b3e:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2019] working-class <UNK> <UNK> ability\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 2] Key:   [2019] working-class <UNK> <UNK> able\n",
      "     Value: 4 occurrences in 4 documents\n",
      "\n",
      "[ 3] Key:   [2019] working-class <UNK> <UNK> aim\n",
      "     Value: 1 occurrences in 1 documents\n",
      "\n",
      "[ 4] Key:   [2019] working-class <UNK> <UNK> also\n",
      "     Value: 16 occurrences in 16 documents\n",
      "\n",
      "[ 5] Key:   [2019] working-class <UNK> <UNK> although\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek(pivoted_db, start_key=\"[2019] working-class <UNK> <UNK> <UNK> <UNK>\", n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c63e70a0ce04fb",
   "metadata": {},
   "source": [
    "## `db_peek_prefix`: Records matching a prefix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inspect_prefix",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T06:40:20.607162Z",
     "start_time": "2025-12-25T06:40:10.425620Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs with prefix 000007d63c554e4b3e20776f726b696e672d636c617373:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2006] <UNK> working-class <UNK> <UNK>\n",
      "     Value: 11,174 occurrences in 10,238 documents\n",
      "\n",
      "[ 2] Key:   [2006] <UNK> working-class <UNK> able\n",
      "     Value: 4 occurrences in 4 documents\n",
      "\n",
      "[ 3] Key:   [2006] <UNK> working-class <UNK> achieve\n",
      "     Value: 3 occurrences in 2 documents\n",
      "\n",
      "[ 4] Key:   [2006] <UNK> working-class <UNK> action\n",
      "     Value: 2 occurrences in 2 documents\n",
      "\n",
      "[ 5] Key:   [2006] <UNK> working-class <UNK> actually\n",
      "     Value: 14 occurrences in 14 documents\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pivoted_db = f'{db_path_stub}{release}/{language}/{ngram_size}gram_files/{ngram_size}grams_pivoted.db'\n",
    "\n",
    "db_peek_prefix(pivoted_db, prefix=\"[2006] <UNK> working-class\", n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a850b70859d225",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (lexichron)",
   "language": "python",
   "name": "lexichron"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
