{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# **Davies Corpus: Acquisition Workflow**\nThis workflow ingests a locally downloaded Davies corpus (e.g., COHA, COCA) into a RocksDB database. Unlike Google Books ngrams which are downloaded on-the-fly, Davies corpora must be obtained separately and stored locally before running this notebook.",
   "id": "e5392ef8bda7b4d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## **Setup**\n",
    "### Imports"
   ],
   "id": "c941ee5a2f74fde8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T02:20:44.543786Z",
     "start_time": "2025-12-16T02:20:43.826651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "from ngramkit.ngram_filter.lemmatizer import CachedSpacyLemmatizer\n",
    "from davieskit.davies_acquire import ingest_davies_corpus\n",
    "from davieskit.davies_filter import filter_davies_corpus, write_whitelist\n",
    "from ngramkit.utilities.peek import db_head, db_peek"
   ],
   "id": "f261bd4a6317873c",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "config_header",
   "metadata": {},
   "source": "### Configure\nHere we set basic parameters: the corpus name, local path to the downloaded corpus files, and database storage paths."
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T02:20:45.121324Z",
     "start_time": "2025-12-16T02:20:44.953970Z"
    }
   },
   "cell_type": "code",
   "source": [
    "corpus_name = 'COHA'\n",
    "db_path_stub = f'/scratch/edk202/NLP_corpora/{corpus_name}/'"
   ],
   "id": "111c835807fdb89",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "phase1_header",
   "metadata": {},
   "source": "## **Ingest Corpus into Database**"
  },
  {
   "cell_type": "code",
   "id": "phase1_run",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T00:15:21.556488Z",
     "start_time": "2025-12-15T00:12:59.483865Z"
    }
   },
   "source": [
    "ingest_davies_corpus(\n",
    "    db_path_stub = db_path_stub,\n",
    "    workers=24,\n",
    "    write_batch_size=500_000,\n",
    "    track_genre=False,\n",
    "    compact_after=True\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHA CORPUS ACQUISITION\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-14 19:12:59\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Corpus path:          /scratch/edk202/NLP_corpora/COHA\n",
      "Text directory:       /scratch/edk202/NLP_corpora/COHA/text\n",
      "DB path:              /scratch/edk202/NLP_corpora/COHA/COHA\n",
      "Text files found:     20\n",
      "Workers:              24\n",
      "Batch size:           500,000\n",
      "Genre tracking:       Disabled\n",
      "\n",
      "Processing Files\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files Processed: 100%|█████████████████████████████████████████████████████████| 20/20 [01:57<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Post-Ingestion Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         1.76 GB\n",
      "Compaction completed in 0:00:20\n",
      "Size before:             1.76 GB\n",
      "Size after:              2.44 GB\n",
      "Space saved:             -691.15 MB (-38.3%)\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Files processed:          20/20\n",
      "Failed files:             0\n",
      "Total sentences written:  23,128,970\n",
      "Database path:            /scratch/edk202/NLP_corpora/COHA/COHA\n",
      "\n",
      "End Time: 2025-12-14 19:15:19\n",
      "Total Runtime: 0:02:19.421905\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## **Filter Database**",
   "id": "6367a394a9898ca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-16T03:26:44.921329Z",
     "start_time": "2025-12-16T03:12:02.373813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "filter_options = {\n",
    "    'stop_set': set(get_stop_words(\"english\")),\n",
    "    'lemma_gen': CachedSpacyLemmatizer()\n",
    "}\n",
    "\n",
    "filter_davies_corpus(\n",
    "    src_db_path=f'{db_path_stub}/{corpus_name}',\n",
    "    dst_db_path=f'{db_path_stub}/{corpus_name}_filtered',\n",
    "    workers=100,\n",
    "    batch_size=250_000,\n",
    "    track_genre=False,\n",
    "    create_whitelist=True,\n",
    "    whitelist_path=f'{db_path_stub}/{corpus_name}_whitelist.txt',\n",
    "    apply_whitelist=True,\n",
    "    whitelist_size=30_000,\n",
    "    whitelist_spell_check=True,\n",
    "    whitelist_year_range=(1900, 2020),\n",
    "    compact_after=True,\n",
    "    **filter_options\n",
    ")"
   ],
   "id": "dcf8950dea7ee308",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COHA CORPUS FILTERING\n",
      "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n",
      "Start Time: 2025-12-15 22:12:02\n",
      "\n",
      "Configuration\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Source DB:            /scratch/edk202/NLP_corpora/COHA/COHA\n",
      "Destination DB:       /scratch/edk202/NLP_corpora/COHA/COHA_filtered\n",
      "Lowercase:            True\n",
      "Alpha only:           True\n",
      "Filter short:         True (min_len=3)\n",
      "Filter stops:         True\n",
      "Apply lemmas:         True\n",
      "Workers:              100\n",
      "Batch size:           250,000\n",
      "\n",
      "Processing Sentences\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches Processed: 100%|███████████████████████████████████████████████████████| 89/89 [00:57<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Building Whitelist\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Whitelist path:          /scratch/edk202/NLP_corpora/COHA/COHA_whitelist.txt\n",
      "Top N tokens:            30,000\n",
      "Year range:              1900-2020\n",
      "Spell check:             True\n",
      "\n",
      "Scanning database...\n",
      "Found 20,645,839 sentences\n",
      "Detecting years present in corpus...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building token frequencies: 100%|██████████████████████████████████| 20645839/20645839 [10:21<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Years present in corpus within range: 11 years\n",
      "  Range: 1900 to 2000\n",
      "  Years: [1900, 1910, 1920, 1930, 1940, 1950, 1960, 1970, 1980, 1990, 2000]\n",
      "\n",
      "Filtering tokens by year coverage (must appear in all 11 years)...\n",
      "Tokens before year filter: 64,598\n",
      "Tokens after year filter:  23,805\n",
      "Tokens removed:            40,793\n",
      "\n",
      "Ranking 23,805 unique tokens...\n",
      "Selected top 30,000 tokens\n",
      "Writing whitelist to /scratch/edk202/NLP_corpora/COHA/COHA_whitelist.txt...\n",
      "Whitelist written successfully: /scratch/edk202/NLP_corpora/COHA/COHA_whitelist.txt\n",
      "\n",
      "Applying Whitelist\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Loading whitelist into memory...\n",
      "Loaded 23,805 tokens from whitelist\n",
      "\n",
      "Replacing non-whitelist tokens with <UNK>...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches Processed: 100%|███████████████████████████████████████████████████████| 83/83 [01:03<00:00]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Whitelist application complete!\n",
      "Sentences processed:      20,645,839\n",
      "Sentences modified:       733,338\n",
      "\n",
      "Post-Filter Compaction\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Initial DB size:         2.39 GB\n",
      "Compaction completed in 0:00:18\n",
      "Size before:             2.39 GB\n",
      "Size after:              2.39 GB\n",
      "Space saved:             253.48 KB (0.0%)\n",
      "\n",
      "\n",
      "Processing complete!\n",
      "\n",
      "Final Summary\n",
      "════════════════════════════════════════════════════════════════════════════════════════════════════\n",
      "Sentences read:           22,172,225\n",
      "Writes accumulated:       20,802,843\n",
      "Sentences rejected:       1,369,382\n",
      "Retention rate:           93.8%\n",
      "Destination DB:           /scratch/edk202/NLP_corpora/COHA/COHA_filtered\n",
      "\n",
      "End Time: 2025-12-15 22:26:41\n",
      "Total Runtime: 0:14:38.559571\n",
      "\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "inspect_header",
   "metadata": {},
   "source": "## **Optional: Inspect Database**"
  },
  {
   "cell_type": "markdown",
   "id": "inspect_head_header",
   "metadata": {},
   "source": "### `db_head`: Show first N records"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T04:04:50.152795Z",
     "start_time": "2025-12-15T04:04:49.939773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/{corpus_name}_filtered'\n",
    "\n",
    "db_head(str(db_path), n=15)"
   ],
   "id": "55493a0831eb239",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 15 key-value pairs:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1810] 1 <UNK> 2 attendant\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 2] Key:   [1810] 1 <UNK> 2 fisherman fish <UNK> <UNK> rock\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 3] Key:   [1810] 1 <UNK> <UNK> <UNK> <UNK> clerk <UNK> norwich <UNK> <UNK> <UNK> partner <UNK> <UNK> general store <UNK> <UNK> con necticut town alway <UNK> everywhere successful\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 4] Key:   [1810] 1 <UNK> <UNK> <UNK> <UNK> educate secondly board thirdly lodge fourthly clothe <UNK> <UNK> <UNK> expence <UNK> sir spendall flinty\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 5] Key:   [1810] 1 <UNK> <UNK> road <UNK> crime 1 <UNK> must <UNK> <UNK> road <UNK> expiation\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 6] Key:   [1810] 1 <UNK> <UNK> sub ject <UNK> <UNK> mediator <UNK> kingdom <UNK> <UNK> right <UNK> use carnal weapon <UNK> defense <UNK> <UNK> <UNK> try situation possible\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 7] Key:   [1810] 1 <UNK> <UNK> tie <UNK> <UNK> purse <UNK> give fourbin <UNK> hundred loui <UNK> take <UNK> madam <UNK> cerval <UNK> <UNK> <UNK> may make <UNK> figure <UNK> <UNK> trial <UNK> <UNK> case <UNK> misfortune <UNK> <UNK> <UNK> quite <UNK> good please <UNK> <UNK> <UNK> find <UNK> <UNK> fourbin\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 8] Key:   [1810] 1 <UNK> say <UNK> leave <UNK> letter <UNK> <UNK> inn\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[ 9] Key:   [1810] 1 exhibition <UNK> <UNK> work <UNK> british artist\n",
      "     Value: Total: 2 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[10] Key:   [1810] 1 fascinate <UNK> vanquish <UNK> <UNK> slay see <UNK> return <UNK> snake <UNK> lose <UNK> day\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[11] Key:   [1810] 1 fisherman\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[12] Key:   [1810] 1 fisherman <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[13] Key:   [1810] 1 fisherman <UNK> <UNK> fish <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> go <UNK> caucu <UNK> <UNK> prayer <UNK> sarve <UNK> will <UNK> bite <UNK> <UNK> forsooth must starve\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[14] Key:   [1810] 1 fisherman something <UNK> gues <UNK> scare <UNK> fish away\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n",
      "[15] Key:   [1810] 1 let <UNK> consider <UNK> <UNK> deep reflection <UNK> <UNK> <UNK> will <UNK> 1 1 <UNK> will <UNK> <UNK> <UNK> stage\n",
      "     Value: Total: 1 occurrences in 1 volumes (1810-1810, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### `db_peek`: Show records starting from a key",
   "id": "bf56e39cb59e741c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T04:05:01.378793Z",
     "start_time": "2025-12-15T04:05:01.293768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/{corpus_name}_filtered'\n",
    "\n",
    "db_peek(db_path, start_key=\"[2000] <UNK> horror\", n=5)\n"
   ],
   "id": "8fbde6180d828266",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 key-value pairs starting from 000007d03c554e4b3e20686f72726f72:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [2000] <UNK> horror <UNK> <UNK> <UNK> <UNK> <UNK> somehow make <UNK> wise wise even <UNK> <UNK> witch <UNK> <UNK> speak <UNK> along <UNK> <UNK> empty easy conversation <UNK> <UNK> <UNK> <UNK> <UNK> old friend <UNK> friend now seem like mirror show <UNK> whatever <UNK> want <UNK> see\n",
      "     Value: Total: 2 occurrences in 2 volumes (2000-2000, 1 bins)\n",
      "\n",
      "[ 2] Key:   [2000] <UNK> horror <UNK> <UNK> <UNK> beard <UNK> good <UNK> <UNK> heart begin <UNK> beat <UNK> fast <UNK> become almost <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (2000-2000, 1 bins)\n",
      "\n",
      "[ 3] Key:   [2000] <UNK> horror <UNK> <UNK> <UNK> happen fully hit <UNK>\n",
      "     Value: Total: 2 occurrences in 2 volumes (2000-2000, 1 bins)\n",
      "\n",
      "[ 4] Key:   [2000] <UNK> horror <UNK> <UNK> choice <UNK> <UNK> <UNK> still\n",
      "     Value: Total: 2 occurrences in 2 volumes (2000-2000, 1 bins)\n",
      "\n",
      "[ 5] Key:   [2000] <UNK> horror <UNK> <UNK> claim <UNK>\n",
      "     Value: Total: 2 occurrences in 2 volumes (2000-2000, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### `db_peek_prefix`: Records matching a prefix",
   "id": "d74c5d5e8f5b6cc5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-15T03:09:49.008912Z",
     "start_time": "2025-12-15T03:09:48.916247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "db_path = f'/scratch/edk202/NLP_corpora/{corpus_name}/{corpus_name}_filtered'\n",
    "\n",
    "db_peek(db_path, start_key=\"[1980] time\", n=15)"
   ],
   "id": "2670fb8721bd9b6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15 key-value pairs starting from 000007bc74696d65:\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "[ 1] Key:   [1980] time <UNK> <UNK> 1 run <UNK> <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 2] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> behavior <UNK> <UNK> quick word <UNK> <UNK> hard look <UNK> disapproval <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 3] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> john <UNK> <UNK> page <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 4] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> life <UNK> death <UNK> comprehend <UNK> language <UNK> dialect <UNK> other\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 5] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> start <UNK> new time\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 6] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> fine midsummer race\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 7] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> become someone <UNK> can decide <UNK> <UNK>\n",
      "     Value: Total: 2 occurrences in 2 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 8] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> hold <UNK> line <UNK>\n",
      "     Value: Total: 2 occurrences in 2 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[ 9] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> matt <UNK> life <UNK> death <UNK> comprehend <UNK> language <UNK> dialect <UNK> other\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[10] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> <UNK> translate <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> page <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[11] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> accompany <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[12] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> america <UNK> fine midsummer race\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[13] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> assure <UNK> <UNK> <UNK> commitment <UNK> <UNK> <UNK> security\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[14] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> assure <UNK> <UNK> <UNK> commitment <UNK> israel <UNK> security\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n",
      "[15] Key:   [1980] time <UNK> <UNK> <UNK> <UNK> cast anchor <UNK> <UNK> village <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> find <UNK> <UNK> <UNK> <UNK> already <UNK> <UNK> <UNK> <UNK> cargo <UNK> rum <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> obtain two <UNK> three year supply <UNK> even catch <UNK> <UNK> <UNK> <UNK> sail <UNK> back <UNK> <UNK> <UNK> <UNK> report <UNK> <UNK> brig <UNK> captain <UNK> <UNK> <UNK> new <UNK> <UNK> <UNK> sell rum <UNK> <UNK> native <UNK> engage <UNK> <UNK> seal <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> attentive patrol fail <UNK> apprehend <UNK>\n",
      "     Value: Total: 1 occurrences in 1 volumes (1980-1980, 1 bins)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "74892a369be1afc6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
